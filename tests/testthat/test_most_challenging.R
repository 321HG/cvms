library(cvms)
context("most_challenging()")

test_that("binomial model works with most_challenging()", {

  # Load data and fold it
  xpectr::set_test_seed(1)
  dat_ready <- participant.scores %>%
    dplyr::mutate(diagnosis = as.factor(diagnosis))
  dat_list <- groupdata2::fold(dat_ready,
    k = 4,
    num_fold_cols = 8,
    cat_col = "diagnosis",
    id_col = "participant"
  )

  CV_binom <- cross_validate(
    dat_list,
    formulas = c("diagnosis ~ score", "diagnosis ~ score + age"),
    fold_cols = paste0(".folds_", 1:8),
    family = "binomial"
  )

  collected_preds <- dplyr::bind_rows(CV_binom$Predictions, .id = "Model") %>%
    dplyr::group_by(.data$Model)

  hard_to_predict_perc <- most_challenging(collected_preds,
    threshold = 0.25,
    threshold_is = "percentage",
    type = "binomial"
  )

  ## Testing 'hard_to_predict_perc'                                         ####
  ## Initially generated by xpectr
  # Testing class
  expect_equal(
    class(hard_to_predict_perc),
    c("tbl_df", "tbl", "data.frame"),
    fixed = TRUE)
  # Testing column values
  expect_equal(
    hard_to_predict_perc[["Model"]],
    c("1", "1", "1", "1", "1", "1", "1", "1", "2", "2", "2", "2",
      "2", "2", "2", "2", "2", "2"),
    fixed = TRUE)
  expect_equal(
    hard_to_predict_perc[["Observation"]],
    c(1, 4, 5, 7, 10, 20, 21, 15, 1, 4, 7, 10, 20, 21, 30, 5, 11,
      15),
    tolerance = 1e-4)
  expect_equal(
    hard_to_predict_perc[["Correct"]],
    c(0, 0, 0, 0, 0, 0, 0, 6, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2),
    tolerance = 1e-4)
  expect_equal(
    hard_to_predict_perc[["Incorrect"]],
    c(8, 8, 8, 8, 8, 8, 8, 2, 8, 8, 8, 8, 8, 8, 8, 6, 6, 6),
    tolerance = 1e-4)
  expect_equal(
    hard_to_predict_perc[["Accuracy"]],
    c(0, 0, 0, 0, 0, 0, 0, 0.75, 0, 0, 0, 0, 0, 0, 0, 0.25, 0.25,
      0.25),
    tolerance = 1e-4)
  expect_equal(
    hard_to_predict_perc[["<="]],
    c(0.78125, 0.78125, 0.78125, 0.78125, 0.78125, 0.78125, 0.78125,
      0.78125, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25,
      0.25),
    tolerance = 1e-4)
  # Testing column names
  expect_equal(
    names(hard_to_predict_perc),
    c("Model", "Observation", "Correct", "Incorrect", "Accuracy",
      "<="),
    fixed = TRUE)
  # Testing column classes
  expect_equal(
    xpectr::element_classes(hard_to_predict_perc),
    c("character", "integer", "integer", "integer", "numeric", "numeric"),
    fixed = TRUE)
  # Testing column types
  expect_equal(
    xpectr::element_types(hard_to_predict_perc),
    c("character", "integer", "integer", "integer", "double", "double"),
    fixed = TRUE)
  # Testing dimensions
  expect_equal(
    dim(hard_to_predict_perc),
    c(18L, 6L))
  # Testing group keys
  expect_equal(
    colnames(dplyr::group_keys(hard_to_predict_perc)),
    character(0),
    fixed = TRUE)
  ## Finished testing 'hard_to_predict_perc'                                ####

  hard_to_predict_score <- most_challenging(collected_preds,threshold = 0.30,
                                            threshold_is = "score",
                                            type = "binomial")

  ## Testing 'hard_to_predict_score'                                        ####
  ## Initially generated by xpectr
  # Testing class
  expect_equal(
    class(hard_to_predict_score),
    c("tbl_df", "tbl", "data.frame"),
    fixed = TRUE)
  # Testing column values
  expect_equal(
    hard_to_predict_score[["Model"]],
    c("1", "1", "1", "1", "1", "1", "1", "2", "2", "2", "2", "2",
      "2", "2", "2", "2", "2"),
    fixed = TRUE)
  expect_equal(
    hard_to_predict_score[["Observation"]],
    c(1, 4, 5, 7, 10, 20, 21, 1, 4, 7, 10, 20, 21, 30, 5, 11, 15),
    tolerance = 1e-4)
  expect_equal(
    hard_to_predict_score[["Correct"]],
    c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2),
    tolerance = 1e-4)
  expect_equal(
    hard_to_predict_score[["Incorrect"]],
    c(8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 6, 6, 6),
    tolerance = 1e-4)
  expect_equal(
    hard_to_predict_score[["Accuracy"]],
    c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.25, 0.25, 0.25),
    tolerance = 1e-4)
  expect_equal(
    hard_to_predict_score[["<="]],
    c(0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3,
      0.3, 0.3, 0.3, 0.3, 0.3),
    tolerance = 1e-4)
  # Testing column names
  expect_equal(
    names(hard_to_predict_score),
    c("Model", "Observation", "Correct", "Incorrect", "Accuracy",
      "<="),
    fixed = TRUE)
  # Testing column classes
  expect_equal(
    xpectr::element_classes(hard_to_predict_score),
    c("character", "integer", "integer", "integer", "numeric", "numeric"),
    fixed = TRUE)
  # Testing column types
  expect_equal(
    xpectr::element_types(hard_to_predict_score),
    c("character", "integer", "integer", "integer", "double", "double"),
    fixed = TRUE)
  # Testing dimensions
  expect_equal(
    dim(hard_to_predict_score),
    c(17L, 6L))
  # Testing group keys
  expect_equal(
    colnames(dplyr::group_keys(hard_to_predict_score)),
    character(0),
    fixed = TRUE)
  ## Finished testing 'hard_to_predict_score'                               ####

})

test_that("multinomial model works with most_challenging()", {

  # Load data and fold it
  xpectr::set_test_seed(1)

  # # Create and fold dataset
  data_mc <- multiclass_probability_tibble(
    num_classes = 3, num_observations = 50,
    apply_softmax = TRUE, FUN = runif,
    class_name = "predictor_"
  )
  class_names <- paste0("class_", c(1, 2, 3))
  data_mc[["target"]] <- factor(sample(
    x = class_names,
    size = 50, replace = TRUE
  ))
  dat <- groupdata2::fold(data_mc, k = 4, num_fold_cols = 8)

  multinom_model_fn <- function(train_data, formula, hyperparameters) {
    nnet::multinom(
      formula = formula, # converted to formula object within fit_model()
      data = train_data
    )
  }

  random_predict_fn <- function(test_data, model, formula, hyperparameters) {
    multiclass_probability_tibble(
      num_classes = 3, num_observations = nrow(test_data),
      apply_softmax = TRUE, FUN = runif,
      class_name = "class_"
    )
  }

  CVmultinomlist <- cross_validate_fn(dat,
    model_fn = multinom_model_fn,
    predict_fn = random_predict_fn,
    formulas = c(
      "target ~ predictor_1 + predictor_2 + predictor_3",
      "target ~ predictor_1"
    ),
    fold_cols = paste0(".folds_", 1:8),
    type = "multinomial"
  )

  collected_preds <- dplyr::bind_rows(CVmultinomlist$Predictions, .id = "Model") %>%
    dplyr::group_by(.data$Model)

  hard_to_predict <- most_challenging(
    collected_preds,
    threshold = 0.15,
    threshold_is = "percentage",
    type = "multinomial"
  )

  ## Testing 'hard_to_predict'                                              ####
  ## Initially generated by xpectr
  # Testing class
  expect_equal(
    class(hard_to_predict),
    c("tbl_df", "tbl", "data.frame"),
    fixed = TRUE)
  # Testing column values
  expect_equal(
    xpectr::smpl(hard_to_predict[["Model"]], n = 30),
    c("1", "1", "1", "1", "1", "1", "1", "1", "1", "1", "1", "2",
      "2", "2", "2", "2", "2", "2", "2", "2", "2", "2", "2", "2",
      "2", "2", "2", "2", "2", "2"),
    fixed = TRUE)
  expect_equal(
    xpectr::smpl(hard_to_predict[["Observation"]], n = 30),
    c(4, 10, 11, 12, 35, 8, 28, 31, 39, 47, 48, 33, 50, 2, 7, 10,
      11, 16, 18, 21, 23, 25, 27, 28, 29, 31, 37, 41, 43, 45),
    tolerance = 1e-4)
  expect_equal(
    xpectr::smpl(hard_to_predict[["Correct"]], n = 30),
    c(0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 2, 2, 2, 2, 2, 2, 2,
      2, 2, 2, 2, 2, 2, 2, 2, 2, 2),
    tolerance = 1e-4)
  expect_equal(
    xpectr::smpl(hard_to_predict[["Incorrect"]], n = 30),
    c(8, 8, 8, 8, 8, 7, 7, 7, 7, 7, 7, 8, 8, 6, 6, 6, 6, 6, 6, 6,
      6, 6, 6, 6, 6, 6, 6, 6, 6, 6),
    tolerance = 1e-4)
  expect_equal(
    xpectr::smpl(hard_to_predict[["Accuracy"]], n = 30),
    c(0, 0, 0, 0, 0, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0,
      0, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25,
      0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25),
    tolerance = 1e-4)
  expect_equal(
    xpectr::smpl(hard_to_predict[["<="]], n = 30),
    c(0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125,
      0.125, 0.125, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25,
      0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25,
      0.25),
    tolerance = 1e-4)
  # Testing column names
  expect_equal(
    names(hard_to_predict),
    c("Model", "Observation", "Correct", "Incorrect", "Accuracy",
      "<="),
    fixed = TRUE)
  # Testing column classes
  expect_equal(
    xpectr::element_classes(hard_to_predict),
    c("character", "integer", "integer", "integer", "numeric", "numeric"),
    fixed = TRUE)
  # Testing column types
  expect_equal(
    xpectr::element_types(hard_to_predict),
    c("character", "integer", "integer", "integer", "double", "double"),
    fixed = TRUE)
  # Testing dimensions
  expect_equal(
    dim(hard_to_predict),
    c(35L, 6L))
  # Testing group keys
  expect_equal(
    colnames(dplyr::group_keys(hard_to_predict)),
    character(0),
    fixed = TRUE)
  ## Finished testing 'hard_to_predict'                                     ####

  hard_to_predict_score <- most_challenging(collected_preds,
                                            threshold = 0.15,
                                            threshold_is = "score",
                                            type = "multinomial")

  ## Testing 'hard_to_predict_score'                                        ####
  ## Initially generated by xpectr
  # Testing class
  expect_equal(
    class(hard_to_predict_score),
    c("tbl_df", "tbl", "data.frame"),
    fixed = TRUE)
  # Testing column values
  expect_equal(
    hard_to_predict_score[["Model"]],
    c("1", "1", "1", "1", "1", "1", "1", "1", "1", "1", "1", "1",
      "1", "1", "2", "2", "2"),
    fixed = TRUE)
  expect_equal(
    hard_to_predict_score[["Observation"]],
    c(4, 10, 11, 12, 17, 35, 6, 8, 28, 31, 33, 39, 47, 48, 5, 33,
      50),
    tolerance = 1e-4)
  expect_equal(
    hard_to_predict_score[["Correct"]],
    c(0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0),
    tolerance = 1e-4)
  expect_equal(
    hard_to_predict_score[["Incorrect"]],
    c(8, 8, 8, 8, 8, 8, 7, 7, 7, 7, 7, 7, 7, 7, 8, 8, 8),
    tolerance = 1e-4)
  expect_equal(
    hard_to_predict_score[["Accuracy"]],
    c(0, 0, 0, 0, 0, 0, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125,
      0.125, 0.125, 0, 0, 0),
    tolerance = 1e-4)
  expect_equal(
    hard_to_predict_score[["<="]],
    c(0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15,
      0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15),
    tolerance = 1e-4)
  # Testing column names
  expect_equal(
    names(hard_to_predict_score),
    c("Model", "Observation", "Correct", "Incorrect", "Accuracy",
      "<="),
    fixed = TRUE)
  # Testing column classes
  expect_equal(
    xpectr::element_classes(hard_to_predict_score),
    c("character", "integer", "integer", "integer", "numeric", "numeric"),
    fixed = TRUE)
  # Testing column types
  expect_equal(
    xpectr::element_types(hard_to_predict_score),
    c("character", "integer", "integer", "integer", "double", "double"),
    fixed = TRUE)
  # Testing dimensions
  expect_equal(
    dim(hard_to_predict_score),
    c(17L, 6L))
  # Testing group keys
  expect_equal(
    colnames(dplyr::group_keys(hard_to_predict_score)),
    character(0),
    fixed = TRUE)
  ## Finished testing 'hard_to_predict_score'                               ####

})

test_that("multinomial - predicted.musicians works with most_challenging()", {

  xpectr::set_test_seed(1)

  hard_to_predict <- most_challenging(
    predicted.musicians %>% dplyr::group_by(.data$Classifier),
    obs_id_col = "ID",
    threshold = 0.15,
    threshold_is = "percentage",
    type = "multinomial"
  )

  ## Testing 'hard_to_predict'                                              ####
  ## Initially generated by xpectr
  # Testing class
  expect_equal(
    class(hard_to_predict),
    c("tbl_df", "tbl", "data.frame"),
    fixed = TRUE)
  # Testing column values
  expect_equal(
    xpectr::smpl(hard_to_predict[["Classifier"]], n = 30),
    c("e1071_svm", "e1071_svm", "e1071_svm", "e1071_svm", "e1071_svm",
      "e1071_svm", "e1071_svm", "e1071_svm", "e1071_svm", "nnet_multinom",
      "nnet_multinom", "nnet_multinom", "nnet_multinom", "nnet_multinom",
      "nnet_multinom", "nnet_multinom", "nnet_multinom", "nnet_multinom",
      "nnet_multinom", "nnet_multinom", "randomForest", "randomForest",
      "randomForest", "randomForest", "randomForest", "randomForest",
      "randomForest", "randomForest", "randomForest", "randomForest"),
    fixed = TRUE)
  expect_equal(
    xpectr::smpl(hard_to_predict[["ID"]], n = 30),
    structure(c(7L, 17L, 22L, 27L, 40L, 48L, 51L, 52L, 59L, 1L, 2L,
      9L, 25L, 27L, 40L, 44L, 51L, 54L, 58L, 60L, 11L, 14L, 25L,
      30L, 43L, 47L, 50L, 51L, 52L, 60L), .Label = c("1", "2", "3",
      "4", "5", "6", "7", "8", "9", "10", "11", "12", "13", "14",
      "15", "16", "17", "18", "19", "20", "21", "22", "23", "24",
      "25", "26", "27", "28", "29", "30", "31", "32", "33", "34",
      "35", "36", "37", "38", "39", "40", "41", "42", "43", "44",
      "45", "46", "47", "48", "49", "50", "51", "52", "53", "54",
      "55", "56", "57", "58", "59", "60"), class = "factor"))
  expect_equal(
    xpectr::smpl(hard_to_predict[["Correct"]], n = 30),
    c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
      0, 0, 0, 0, 0, 0, 0, 0, 0, 0),
    tolerance = 1e-4)
  expect_equal(
    xpectr::smpl(hard_to_predict[["Incorrect"]], n = 30),
    c(3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
      3, 3, 3, 3, 3, 3, 3, 3, 3, 3),
    tolerance = 1e-4)
  expect_equal(
    xpectr::smpl(hard_to_predict[["Accuracy"]], n = 30),
    c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
      0, 0, 0, 0, 0, 0, 0, 0, 0, 0),
    tolerance = 1e-4)
  expect_equal(
    xpectr::smpl(hard_to_predict[["<="]], n = 30),
    c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
      0, 0, 0, 0, 0, 0, 0, 0, 0, 0),
    tolerance = 1e-4)
  # Testing column names
  expect_equal(
    names(hard_to_predict),
    c("Classifier", "ID", "Correct", "Incorrect", "Accuracy", "<="),
    fixed = TRUE)
  # Testing column classes
  expect_equal(
    xpectr::element_classes(hard_to_predict),
    c("character", "factor", "integer", "integer", "numeric", "numeric"),
    fixed = TRUE)
  # Testing column types
  expect_equal(
    xpectr::element_types(hard_to_predict),
    c("character", "integer", "integer", "integer", "double", "double"),
    fixed = TRUE)
  # Testing dimensions
  expect_equal(
    dim(hard_to_predict),
    c(67L, 6L))
  # Testing group keys
  expect_equal(
    colnames(dplyr::group_keys(hard_to_predict)),
    character(0),
    fixed = TRUE)
  ## Finished testing 'hard_to_predict'                                     ####

  hard_to_predict_score <- most_challenging(
    predicted.musicians %>% dplyr::group_by(.data$Classifier),
    obs_id_col = "ID",
    threshold = 0.4,
    threshold_is = "score",
    type = "multinomial"
  )

  ## Testing 'hard_to_predict_score'                                        ####
  ## Initially generated by xpectr
  # Testing class
  expect_equal(
    class(hard_to_predict_score),
    c("tbl_df", "tbl", "data.frame"),
    fixed = TRUE)
  # Testing column values
  expect_equal(
    xpectr::smpl(hard_to_predict_score[["Classifier"]], n = 30),
    c("e1071_svm", "e1071_svm", "e1071_svm", "e1071_svm", "e1071_svm",
      "e1071_svm", "e1071_svm", "nnet_multinom", "nnet_multinom",
      "nnet_multinom", "nnet_multinom", "nnet_multinom", "nnet_multinom",
      "nnet_multinom", "nnet_multinom", "nnet_multinom", "nnet_multinom",
      "randomForest", "randomForest", "randomForest", "randomForest",
      "randomForest", "randomForest", "randomForest", "randomForest",
      "randomForest", "randomForest", "randomForest", "randomForest",
      "randomForest"),
    fixed = TRUE)
  expect_equal(
    xpectr::smpl(hard_to_predict_score[["ID"]], n = 30),
    structure(c(22L, 29L, 30L, 37L, 59L, 26L, 50L, 17L, 22L, 24L,
      30L, 44L, 12L, 35L, 39L, 52L, 53L, 1L, 14L, 25L, 29L, 43L,
      49L, 50L, 51L, 60L, 12L, 15L, 40L, 41L), .Label = c("1", "2",
      "3", "4", "5", "6", "7", "8", "9", "10", "11", "12", "13",
      "14", "15", "16", "17", "18", "19", "20", "21", "22", "23",
      "24", "25", "26", "27", "28", "29", "30", "31", "32", "33",
      "34", "35", "36", "37", "38", "39", "40", "41", "42", "43",
      "44", "45", "46", "47", "48", "49", "50", "51", "52", "53",
      "54", "55", "56", "57", "58", "59", "60"), class = "factor"))
  expect_equal(
    xpectr::smpl(hard_to_predict_score[["Correct"]], n = 30),
    c(0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0,
      0, 0, 0, 0, 0, 0, 1, 1, 1, 1),
    tolerance = 1e-4)
  expect_equal(
    xpectr::smpl(hard_to_predict_score[["Incorrect"]], n = 30),
    c(3, 3, 3, 3, 3, 2, 2, 3, 3, 3, 3, 3, 2, 2, 2, 2, 2, 3, 3, 3,
      3, 3, 3, 3, 3, 3, 2, 2, 2, 2),
    tolerance = 1e-4)
  expect_equal(
    xpectr::smpl(hard_to_predict_score[["Accuracy"]], n = 30),
    c(0, 0, 0, 0, 0, 0.33333, 0.33333, 0, 0, 0, 0, 0, 0.33333, 0.33333,
      0.33333, 0.33333, 0.33333, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.33333,
      0.33333, 0.33333, 0.33333),
    tolerance = 1e-4)
  expect_equal(
    xpectr::smpl(hard_to_predict_score[["<="]], n = 30),
    c(0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4,
      0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4,
      0.4, 0.4, 0.4, 0.4, 0.4, 0.4),
    tolerance = 1e-4)
  # Testing column names
  expect_equal(
    names(hard_to_predict_score),
    c("Classifier", "ID", "Correct", "Incorrect", "Accuracy", "<="),
    fixed = TRUE)
  # Testing column classes
  expect_equal(
    xpectr::element_classes(hard_to_predict_score),
    c("character", "factor", "integer", "integer", "numeric", "numeric"),
    fixed = TRUE)
  # Testing column types
  expect_equal(
    xpectr::element_types(hard_to_predict_score),
    c("character", "integer", "integer", "integer", "double", "double"),
    fixed = TRUE)
  # Testing dimensions
  expect_equal(
    dim(hard_to_predict_score),
    c(97L, 6L))
  # Testing group keys
  expect_equal(
    colnames(dplyr::group_keys(hard_to_predict_score)),
    character(0),
    fixed = TRUE)
  ## Finished testing 'hard_to_predict_score'                               ####

  hard_to_predict_perc_non_grouped <- most_challenging(
    predicted.musicians,
    obs_id_col = "ID",
    threshold = 0.30,
    threshold_is = "percentage",
    type = "multinomial"
  )

  ## Testing 'hard_to_predict_perc_non_grouped'                             ####
  ## Initially generated by xpectr
  # Assigning output
  output_17375 <- hard_to_predict_perc_non_grouped
  # Testing class
  expect_equal(
    class(output_17375),
    c("tbl_df", "tbl", "data.frame"),
    fixed = TRUE)
  # Testing column values
  expect_equal(
    output_17375[["ID"]],
    structure(c(1L, 2L, 9L, 25L, 29L, 30L, 37L, 43L, 48L, 51L, 54L,
      58L, 59L, 60L, 7L, 14L, 22L, 27L, 35L, 40L, 52L), .Label = c("1",
      "2", "3", "4", "5", "6", "7", "8", "9", "10", "11", "12",
      "13", "14", "15", "16", "17", "18", "19", "20", "21", "22",
      "23", "24", "25", "26", "27", "28", "29", "30", "31", "32",
      "33", "34", "35", "36", "37", "38", "39", "40", "41", "42",
      "43", "44", "45", "46", "47", "48", "49", "50", "51", "52",
      "53", "54", "55", "56", "57", "58", "59", "60"), class = "factor"))
  expect_equal(
    output_17375[["Correct"]],
    c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1,
      1),
    tolerance = 1e-4)
  expect_equal(
    output_17375[["Incorrect"]],
    c(9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 8, 8, 8, 8, 8, 8,
      8),
    tolerance = 1e-4)
  expect_equal(
    output_17375[["Accuracy"]],
    c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.11111, 0.11111,
      0.11111, 0.11111, 0.11111, 0.11111, 0.11111),
    tolerance = 1e-4)
  expect_equal(
    output_17375[["<="]],
    c(0.11111, 0.11111, 0.11111, 0.11111, 0.11111, 0.11111, 0.11111,
      0.11111, 0.11111, 0.11111, 0.11111, 0.11111, 0.11111, 0.11111,
      0.11111, 0.11111, 0.11111, 0.11111, 0.11111, 0.11111, 0.11111),
    tolerance = 1e-4)
  # Testing column names
  expect_equal(
    names(output_17375),
    c("ID", "Correct", "Incorrect", "Accuracy", "<="),
    fixed = TRUE)
  # Testing column classes
  expect_equal(
    xpectr::element_classes(output_17375),
    c("factor", "integer", "integer", "numeric", "numeric"),
    fixed = TRUE)
  # Testing column types
  expect_equal(
    xpectr::element_types(output_17375),
    c("integer", "integer", "integer", "double", "double"),
    fixed = TRUE)
  # Testing dimensions
  expect_equal(
    dim(output_17375),
    c(21L, 5L))
  # Testing group keys
  expect_equal(
    colnames(dplyr::group_keys(output_17375)),
    character(0),
    fixed = TRUE)
  ## Finished testing 'hard_to_predict_perc_non_grouped'                    ####


  hard_to_predict_score_non_grouped <- most_challenging(
    predicted.musicians,
    obs_id_col = "ID",
    threshold = 0.4,
    threshold_is = "score",
    type = "multinomial"
  )

  ## Testing 'hard_to_predict_score_non_grouped'                            ####
  ## Initially generated by xpectr
  # Testing class
  expect_equal(
    class(hard_to_predict_score_non_grouped),
    c("tbl_df", "tbl", "data.frame"),
    fixed = TRUE)
  # Testing column values
  expect_equal(
    hard_to_predict_score_non_grouped[["ID"]],
    structure(c(1L, 2L, 9L, 25L, 29L, 30L, 37L, 43L, 48L, 51L, 54L,
      58L, 59L, 60L, 7L, 14L, 22L, 27L, 35L, 40L, 52L, 11L, 47L,
      4L, 17L, 44L, 49L, 53L), .Label = c("1", "2", "3", "4", "5",
      "6", "7", "8", "9", "10", "11", "12", "13", "14", "15", "16",
      "17", "18", "19", "20", "21", "22", "23", "24", "25", "26",
      "27", "28", "29", "30", "31", "32", "33", "34", "35", "36",
      "37", "38", "39", "40", "41", "42", "43", "44", "45", "46",
      "47", "48", "49", "50", "51", "52", "53", "54", "55", "56",
      "57", "58", "59", "60"), class = "factor"))
  expect_equal(
    hard_to_predict_score_non_grouped[["Correct"]],
    c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1,
      1, 2, 2, 3, 3, 3, 3, 3),
    tolerance = 1e-4)
  expect_equal(
    hard_to_predict_score_non_grouped[["Incorrect"]],
    c(9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 8, 8, 8, 8, 8, 8,
      8, 7, 7, 6, 6, 6, 6, 6),
    tolerance = 1e-4)
  expect_equal(
    hard_to_predict_score_non_grouped[["Accuracy"]],
    c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.11111, 0.11111,
      0.11111, 0.11111, 0.11111, 0.11111, 0.11111, 0.22222, 0.22222,
      0.33333, 0.33333, 0.33333, 0.33333, 0.33333),
    tolerance = 1e-4)
  expect_equal(
    hard_to_predict_score_non_grouped[["<="]],
    c(0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4,
      0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4,
      0.4, 0.4, 0.4, 0.4),
    tolerance = 1e-4)
  # Testing column names
  expect_equal(
    names(hard_to_predict_score_non_grouped),
    c("ID", "Correct", "Incorrect", "Accuracy", "<="),
    fixed = TRUE)
  # Testing column classes
  expect_equal(
    xpectr::element_classes(hard_to_predict_score_non_grouped),
    c("factor", "integer", "integer", "numeric", "numeric"),
    fixed = TRUE)
  # Testing column types
  expect_equal(
    xpectr::element_types(hard_to_predict_score_non_grouped),
    c("integer", "integer", "integer", "double", "double"),
    fixed = TRUE)
  # Testing dimensions
  expect_equal(
    dim(hard_to_predict_score_non_grouped),
    c(28L, 5L))
  # Testing group keys
  expect_equal(
    colnames(dplyr::group_keys(hard_to_predict_score_non_grouped)),
    character(0),
    fixed = TRUE)
  ## Finished testing 'hard_to_predict_score_non_grouped'                   ####

})

test_that("gaussian model works with most_challenging()", {

  # Load data and fold it
  xpectr::set_test_seed(1)
  dat_ready <- participant.scores %>%
    dplyr::mutate(diagnosis = as.factor(diagnosis))
  dat_list <- groupdata2::fold(dat_ready,
    k = 4,
    num_fold_cols = 8,
    cat_col = "diagnosis",
    id_col = "participant"
  )

  CV_gauss <- cross_validate(
    dat_list,
    formulas = c(
      "score ~ diagnosis + session",
      "score ~ diagnosis + age + session"
    ),
    fold_cols = paste0(".folds_", 1:8),
    family = "gaussian"
  )

  collected_preds <- dplyr::bind_rows(CV_gauss$Predictions, .id = "Model") %>%
    dplyr::group_by(.data$Model)

  hard_to_predict_perc <- most_challenging(collected_preds,
    type = "gaussian",
    threshold = 0.2, threshold_is = "percentage"
  )

  ## Testing 'hard_to_predict_perc'                                         ####
  ## Initially generated by xpectr
  # Testing class
  expect_equal(
    class(hard_to_predict_perc),
    c("tbl_df", "tbl", "data.frame"),
    fixed = TRUE)
  # Testing column values
  expect_equal(
    hard_to_predict_perc[["Model"]],
    c("1", "1", "1", "1", "1", "1", "2", "2", "2", "2", "2", "2"),
    fixed = TRUE)
  expect_equal(
    hard_to_predict_perc[["Observation"]],
    c(20, 24, 21, 3, 5, 19, 20, 24, 21, 27, 3, 9),
    tolerance = 1e-4)
  expect_equal(
    hard_to_predict_perc[["MAE"]],
    c(26.61042, 20.93735, 18.2131, 14.77034, 13.33333, 13.00774, 26.76931,
      21.74228, 18.37199, 13.48558, 13.89225, 13.74331),
    tolerance = 1e-4)
  expect_equal(
    hard_to_predict_perc[["RMSE"]],
    c(26.61253, 20.95156, 18.22234, 14.78232, 13.33333, 13.01421,
      26.77572, 21.78456, 18.39316, 14.71466, 13.953, 13.82658),
    tolerance = 1e-4)
  expect_equal(
    hard_to_predict_perc[[">="]],
    c(12.61127, 12.61127, 12.61127, 12.61127, 12.61127, 12.61127,
      13.30434, 13.30434, 13.30434, 13.30434, 13.30434, 13.30434),
    tolerance = 1e-4)
  # Testing column names
  expect_equal(
    names(hard_to_predict_perc),
    c("Model", "Observation", "MAE", "RMSE", ">="),
    fixed = TRUE)
  # Testing column classes
  expect_equal(
    xpectr::element_classes(hard_to_predict_perc),
    c("character", "integer", "numeric", "numeric", "numeric"),
    fixed = TRUE)
  # Testing column types
  expect_equal(
    xpectr::element_types(hard_to_predict_perc),
    c("character", "integer", "double", "double", "double"),
    fixed = TRUE)
  # Testing dimensions
  expect_equal(
    dim(hard_to_predict_perc),
    c(12L, 5L))
  # Testing group keys
  expect_equal(
    colnames(dplyr::group_keys(hard_to_predict_perc)),
    character(0),
    fixed = TRUE)
  ## Finished testing 'hard_to_predict_perc'                                ####

  hard_to_predict_score <- most_challenging(collected_preds,
    type = "gaussian",
    threshold = 13.9, threshold_is = "score"
  )

  ## Testing 'hard_to_predict_score'                                        ####
  ## Initially generated by xpectr
  # Testing class
  expect_equal(
    class(hard_to_predict_score),
    c("tbl_df", "tbl", "data.frame"),
    fixed = TRUE)
  # Testing column values
  expect_equal(
    hard_to_predict_score[["Model"]],
    c("1", "1", "1", "1", "2", "2", "2", "2", "2"),
    fixed = TRUE)
  expect_equal(
    hard_to_predict_score[["Observation"]],
    c(20, 24, 21, 3, 20, 24, 21, 27, 3),
    tolerance = 1e-4)
  expect_equal(
    hard_to_predict_score[["MAE"]],
    c(26.61042, 20.93735, 18.2131, 14.77034, 26.76931, 21.74228, 18.37199,
      13.48558, 13.89225),
    tolerance = 1e-4)
  expect_equal(
    hard_to_predict_score[["RMSE"]],
    c(26.61253, 20.95156, 18.22234, 14.78232, 26.77572, 21.78456,
      18.39316, 14.71466, 13.953),
    tolerance = 1e-4)
  expect_equal(
    hard_to_predict_score[[">="]],
    c(13.9, 13.9, 13.9, 13.9, 13.9, 13.9, 13.9, 13.9, 13.9),
    tolerance = 1e-4)
  # Testing column names
  expect_equal(
    names(hard_to_predict_score),
    c("Model", "Observation", "MAE", "RMSE", ">="),
    fixed = TRUE)
  # Testing column classes
  expect_equal(
    xpectr::element_classes(hard_to_predict_score),
    c("character", "integer", "numeric", "numeric", "numeric"),
    fixed = TRUE)
  # Testing column types
  expect_equal(
    xpectr::element_types(hard_to_predict_score),
    c("character", "integer", "double", "double", "double"),
    fixed = TRUE)
  # Testing dimensions
  expect_equal(
    dim(hard_to_predict_score),
    c(9L, 5L))
  # Testing group keys
  expect_equal(
    colnames(dplyr::group_keys(hard_to_predict_score)),
    character(0),
    fixed = TRUE)
  ## Finished testing 'hard_to_predict_score'                               ####

})

test_that("exceeds_threshold() works", {

  df <- data.frame(
    "Grp" = rep(c(1,2), each = 5),
    "Observation" = rep(c(1,2,3,4,5), 2),
    "SomeMetric" = c(0.2,0.4,0.6,0.8,0.9,
                     0.3,0.2,0.1,0.8,0.7)
  )

  # xpectr::gxs_function(fn = exceeds_threshold,
  #                      args_values = list(
  #                        "data" = list(df, dplyr::group_by(df,.data$Grp), NA, list("a" = 3)),
  #                        "threshold" = list(0.3, 1, 0, NA, c(0.2, 0.4)),
  #                        "threshold_is" = list("percentage", "score", "hei", NA),
  #                        "metric_name" = list("SomeMetric", "no", NA, 3),
  #                        "maximize" = list(TRUE, FALSE, NA, 1, "TRUE"),
  #                        "grouping_keys" = c(data.frame(),
  #                                            dplyr::group_keys(dplyr::group_by(df,.data$Grp)),
  #                                            NA)
  #                      ))

  ## Testing 'exceeds_threshold'                                              ####
  ## Initially generated by xpectr
  # Testing different combinations of argument values

  # Testing exceeds_threshold(data = dplyr::group_by(df, ....
  # Testing side effects
  expect_error(
    xpectr::strip_msg(exceeds_threshold(data = dplyr::group_by(df, .data$Grp), threshold = 0.3, threshold_is = "percentage", metric_name = "SomeMetric", maximize = TRUE, grouping_keys = data.frame())),
    xpectr::strip("1 assertions failed:\n * 'data' cannot be grouped at this stage."),
    fixed = TRUE)

  # Testing exceeds_threshold(data = df, threshold = 0.3, ...
  # Testing side effects
  expect_error(
    xpectr::strip_msg(exceeds_threshold(data = df, threshold = 0.3, threshold_is = NA, metric_name = "SomeMetric", maximize = TRUE, grouping_keys = data.frame())),
    xpectr::strip(paste0("1 assertions failed:\n * Variable 'threshold_is': Must be el",
           "ement of set {'percentage','score'}, but is 'NA'.")),
    fixed = TRUE)

  # Testing exceeds_threshold(data = df, threshold = 0.3, ...
  # Testing side effects
  expect_error(
    xpectr::strip_msg(exceeds_threshold(data = df, threshold = 0.3, threshold_is = "percentage", metric_name = "no", maximize = TRUE, grouping_keys = data.frame())),
    xpectr::strip(paste0("1 assertions failed:\n * Variable 'colnames(data)': Must inc",
           "lude the elements {no}.")),
    fixed = TRUE)

  # Testing exceeds_threshold(data = df, threshold = 0.3, ...
  # Testing side effects
  expect_error(
    xpectr::strip_msg(exceeds_threshold(data = df, threshold = 0.3, threshold_is = "percentage", metric_name = NA, maximize = TRUE, grouping_keys = data.frame())),
    xpectr::strip(paste0("Assertion on 'must.include' failed. May not contain missing ",
           "values, first at position 1.")),
    fixed = TRUE)

  # Testing exceeds_threshold(data = df, threshold = 0.3, ...
  # Testing side effects
  expect_error(
    xpectr::strip_msg(exceeds_threshold(data = df, threshold = 0.3, threshold_is = "percentage", metric_name = 3, maximize = TRUE, grouping_keys = data.frame())),
    xpectr::strip(paste0("1 assertions failed:\n * Variable 'colnames(data)': Must inc",
           "lude the elements {3}.")),
    fixed = TRUE)

  # Testing exceeds_threshold(data = df, threshold = 0.3, ...
  # Assigning output
  output_14346 <- exceeds_threshold(data = df, threshold = 0.3, threshold_is = "percentage", metric_name = "SomeMetric", maximize = FALSE, grouping_keys = data.frame())
  # Testing class
  expect_equal(
    class(output_14346),
    "data.frame",
    fixed = TRUE)
  # Testing column values
  expect_equal(
    output_14346[["Grp"]],
    c(1, 1, 2),
    tolerance = 1e-4)
  expect_equal(
    output_14346[["Observation"]],
    c(4, 5, 4),
    tolerance = 1e-4)
  expect_equal(
    output_14346[["SomeMetric"]],
    c(0.8, 0.9, 0.8),
    tolerance = 1e-4)
  expect_equal(
    output_14346[["Threshold"]],
    c(0.73, 0.73, 0.73),
    tolerance = 1e-4)
  # Testing column names
  expect_equal(
    names(output_14346),
    c("Grp", "Observation", "SomeMetric", "Threshold"),
    fixed = TRUE)
  # Testing column classes
  expect_equal(
    xpectr::element_classes(output_14346),
    c("numeric", "numeric", "numeric", "numeric"),
    fixed = TRUE)
  # Testing column types
  expect_equal(
    xpectr::element_types(output_14346),
    c("double", "double", "double", "double"),
    fixed = TRUE)
  # Testing dimensions
  expect_equal(
    dim(output_14346),
    3:4)
  # Testing group keys
  expect_equal(
    colnames(dplyr::group_keys(output_14346)),
    character(0),
    fixed = TRUE)

  # Testing exceeds_threshold(data = df, threshold = 0.3, ...
  # Testing side effects
  expect_error(
    xpectr::strip_msg(exceeds_threshold(data = df, threshold = 0.3, threshold_is = "percentage", metric_name = "SomeMetric", maximize = NA, grouping_keys = data.frame())),
    xpectr::strip("1 assertions failed:\n * Variable 'maximize': May not be NA."),
    fixed = TRUE)

  # Testing exceeds_threshold(data = df, threshold = 0.3, ...
  # Testing side effects
  expect_error(
    xpectr::strip_msg(exceeds_threshold(data = df, threshold = 0.3, threshold_is = "percentage", metric_name = "SomeMetric", maximize = 1, grouping_keys = data.frame())),
    xpectr::strip(paste0("1 assertions failed:\n * Variable 'maximize': Must be of typ",
           "e 'logical flag', not 'double'.")),
    fixed = TRUE)

  # Testing exceeds_threshold(data = df, threshold = 0.3, ...
  # Testing side effects
  expect_error(
    xpectr::strip_msg(exceeds_threshold(data = df, threshold = 0.3, threshold_is = "percentage", metric_name = "SomeMetric", maximize = "TRUE", grouping_keys = data.frame())),
    xpectr::strip(paste0("1 assertions failed:\n * Variable 'maximize': Must be of typ",
           "e 'logical flag', not 'character'.")),
    fixed = TRUE)

  # Testing exceeds_threshold(data = df, threshold = 0.3, ...
  # Assigning output
  output_17124 <- exceeds_threshold(data = df, threshold = 0.3, threshold_is = "percentage", metric_name = "SomeMetric", maximize = TRUE, grouping_keys = dplyr::group_keys(dplyr::group_by(df, .data$Grp)))
  # Testing class
  expect_equal(
    class(output_17124),
    "data.frame",
    fixed = TRUE)
  # Testing column values
  expect_equal(
    output_17124[["Grp"]],
    c(1, 1, 2, 2),
    tolerance = 1e-4)
  expect_equal(
    output_17124[["Observation"]],
    c(1, 2, 2, 3),
    tolerance = 1e-4)
  expect_equal(
    output_17124[["SomeMetric"]],
    c(0.2, 0.4, 0.2, 0.1),
    tolerance = 1e-4)
  expect_equal(
    output_17124[["Threshold"]],
    c(0.44, 0.44, 0.22, 0.22),
    tolerance = 1e-4)
  # Testing column names
  expect_equal(
    names(output_17124),
    c("Grp", "Observation", "SomeMetric", "Threshold"),
    fixed = TRUE)
  # Testing column classes
  expect_equal(
    xpectr::element_classes(output_17124),
    c("numeric", "numeric", "numeric", "numeric"),
    fixed = TRUE)
  # Testing column types
  expect_equal(
    xpectr::element_types(output_17124),
    c("double", "double", "double", "double"),
    fixed = TRUE)
  # Testing dimensions
  expect_equal(
    dim(output_17124),
    c(4L, 4L))
  # Testing group keys
  expect_equal(
    colnames(dplyr::group_keys(output_17124)),
    character(0),
    fixed = TRUE)

  # Testing exceeds_threshold(data = df, threshold = 0.3, ...
  # Testing side effects
  expect_error(
    xpectr::strip_msg(exceeds_threshold(data = df, threshold = 0.3, threshold_is = "percentage", metric_name = "SomeMetric", maximize = TRUE, grouping_keys = NA)),
    xpectr::strip(paste0("1 assertions failed:\n * Variable 'grouping_keys': Must be o",
           "f type 'data.frame', not 'logical'.")),
    fixed = TRUE)

  # Testing exceeds_threshold(data = NA, threshold = 0.3, ...
  # Testing side effects
  expect_error(
    xpectr::strip_msg(exceeds_threshold(data = NA, threshold = 0.3, threshold_is = "percentage", metric_name = "SomeMetric", maximize = TRUE, grouping_keys = data.frame())),
    xpectr::strip(paste0("1 assertions failed:\n * Variable 'data': Must be of type 'd",
           "ata.frame', not 'logical'.")),
    fixed = TRUE)

  # Testing exceeds_threshold(data = list(a = 3), threshol...
  # Testing side effects
  expect_error(
    xpectr::strip_msg(exceeds_threshold(data = list(a = 3), threshold = 0.3, threshold_is = "percentage", metric_name = "SomeMetric", maximize = TRUE, grouping_keys = data.frame())),
    xpectr::strip(paste0("1 assertions failed:\n * Variable 'data': Must be of type 'd",
           "ata.frame', not 'list'.")),
    fixed = TRUE)

  # Testing exceeds_threshold(data = df, threshold = 1, th...
  # Assigning output
  output_13999 <- exceeds_threshold(data = df, threshold = 1, threshold_is = "percentage", metric_name = "SomeMetric", maximize = TRUE, grouping_keys = data.frame())
  # Testing class
  expect_equal(
    class(output_13999),
    "data.frame",
    fixed = TRUE)
  # Testing column values
  expect_equal(
    output_13999[["Grp"]],
    c(1, 1, 1, 1, 1, 2, 2, 2, 2, 2),
    tolerance = 1e-4)
  expect_equal(
    output_13999[["Observation"]],
    c(1, 2, 3, 4, 5, 1, 2, 3, 4, 5),
    tolerance = 1e-4)
  expect_equal(
    output_13999[["SomeMetric"]],
    c(0.2, 0.4, 0.6, 0.8, 0.9, 0.3, 0.2, 0.1, 0.8, 0.7),
    tolerance = 1e-4)
  expect_equal(
    output_13999[["Threshold"]],
    c(0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9),
    tolerance = 1e-4)
  # Testing column names
  expect_equal(
    names(output_13999),
    c("Grp", "Observation", "SomeMetric", "Threshold"),
    fixed = TRUE)
  # Testing column classes
  expect_equal(
    xpectr::element_classes(output_13999),
    c("numeric", "numeric", "numeric", "numeric"),
    fixed = TRUE)
  # Testing column types
  expect_equal(
    xpectr::element_types(output_13999),
    c("double", "double", "double", "double"),
    fixed = TRUE)
  # Testing dimensions
  expect_equal(
    dim(output_13999),
    c(10L, 4L))
  # Testing group keys
  expect_equal(
    colnames(dplyr::group_keys(output_13999)),
    character(0),
    fixed = TRUE)

  # Testing exceeds_threshold(data = df, threshold = 0, th...
  # Assigning output
  output_13253 <- exceeds_threshold(data = df, threshold = 0, threshold_is = "percentage", metric_name = "SomeMetric", maximize = TRUE, grouping_keys = data.frame())
  # Testing class
  expect_equal(
    class(output_13253),
    "data.frame",
    fixed = TRUE)
  # Testing column values
  expect_equal(
    output_13253[["Grp"]],
    2,
    tolerance = 1e-4)
  expect_equal(
    output_13253[["Observation"]],
    3,
    tolerance = 1e-4)
  expect_equal(
    output_13253[["SomeMetric"]],
    0.1,
    tolerance = 1e-4)
  expect_equal(
    output_13253[["Threshold"]],
    0.1,
    tolerance = 1e-4)
  # Testing column names
  expect_equal(
    names(output_13253),
    c("Grp", "Observation", "SomeMetric", "Threshold"),
    fixed = TRUE)
  # Testing column classes
  expect_equal(
    xpectr::element_classes(output_13253),
    c("numeric", "numeric", "numeric", "numeric"),
    fixed = TRUE)
  # Testing column types
  expect_equal(
    xpectr::element_types(output_13253),
    c("double", "double", "double", "double"),
    fixed = TRUE)
  # Testing dimensions
  expect_equal(
    dim(output_13253),
    c(1L, 4L))
  # Testing group keys
  expect_equal(
    colnames(dplyr::group_keys(output_13253)),
    character(0),
    fixed = TRUE)

  # Testing exceeds_threshold(data = df, threshold = NA, t...
  # Testing side effects
  expect_error(
    xpectr::strip_msg(exceeds_threshold(data = df, threshold = NA, threshold_is = "percentage", metric_name = "SomeMetric", maximize = TRUE, grouping_keys = data.frame())),
    xpectr::strip("1 assertions failed:\n * Variable 'threshold': May not be NA."),
    fixed = TRUE)

  # Testing exceeds_threshold(data = df, threshold = c(0.2...
  # Testing side effects
  expect_error(
    xpectr::strip_msg(exceeds_threshold(data = df, threshold = c(0.2, 0.4), threshold_is = "percentage", metric_name = "SomeMetric", maximize = TRUE, grouping_keys = data.frame())),
    xpectr::strip("1 assertions failed:\n * Variable 'threshold': Must have length 1."),
    fixed = TRUE)

  # Testing exceeds_threshold(data = df, threshold = 0.3, ...
  # Assigning output
  output_17570 <- exceeds_threshold(data = df, threshold = 0.3, threshold_is = "score", metric_name = "SomeMetric", maximize = TRUE, grouping_keys = data.frame())
  # Testing class
  expect_equal(
    class(output_17570),
    "data.frame",
    fixed = TRUE)
  # Testing column values
  expect_equal(
    output_17570[["Grp"]],
    c(1, 2, 2, 2),
    tolerance = 1e-4)
  expect_equal(
    output_17570[["Observation"]],
    c(1, 1, 2, 3),
    tolerance = 1e-4)
  expect_equal(
    output_17570[["SomeMetric"]],
    c(0.2, 0.3, 0.2, 0.1),
    tolerance = 1e-4)
  expect_equal(
    output_17570[["Threshold"]],
    c(0.3, 0.3, 0.3, 0.3),
    tolerance = 1e-4)
  # Testing column names
  expect_equal(
    names(output_17570),
    c("Grp", "Observation", "SomeMetric", "Threshold"),
    fixed = TRUE)
  # Testing column classes
  expect_equal(
    xpectr::element_classes(output_17570),
    c("numeric", "numeric", "numeric", "numeric"),
    fixed = TRUE)
  # Testing column types
  expect_equal(
    xpectr::element_types(output_17570),
    c("double", "double", "double", "double"),
    fixed = TRUE)
  # Testing dimensions
  expect_equal(
    dim(output_17570),
    c(4L, 4L))
  # Testing group keys
  expect_equal(
    colnames(dplyr::group_keys(output_17570)),
    character(0),
    fixed = TRUE)

  # Testing exceeds_threshold(data = df, threshold = 0.3, ...
  # Testing side effects
  expect_error(
    xpectr::strip_msg(exceeds_threshold(data = df, threshold = 0.3, threshold_is = "hei", metric_name = "SomeMetric", maximize = TRUE, grouping_keys = data.frame())),
    xpectr::strip(paste0("1 assertions failed:\n * Variable 'threshold_is': Must be el",
           "ement of set {'percentage','score'}, but is 'hei'.")),
    fixed = TRUE)

  # Testing exceeds_threshold(data = NULL, threshold = 0.3...
  # Testing side effects
  expect_error(
    xpectr::strip_msg(exceeds_threshold(data = NULL, threshold = 0.3, threshold_is = "percentage", metric_name = "SomeMetric", maximize = TRUE, grouping_keys = data.frame())),
    xpectr::strip(paste0("1 assertions failed:\n * Variable 'data': Must be of type 'd",
           "ata.frame', not 'NULL'.")),
    fixed = TRUE)

  # Testing exceeds_threshold(data = df, threshold = 0.3, ...
  # Assigning output
  output_12026 <- exceeds_threshold(data = df, threshold = 0.3, threshold_is = "percentage", metric_name = "SomeMetric", maximize = TRUE, grouping_keys = data.frame())
  # Testing class
  expect_equal(
    class(output_12026),
    "data.frame",
    fixed = TRUE)
  # Testing column values
  expect_equal(
    output_12026[["Grp"]],
    c(1, 2, 2),
    tolerance = 1e-4)
  expect_equal(
    output_12026[["Observation"]],
    c(1, 2, 3),
    tolerance = 1e-4)
  expect_equal(
    output_12026[["SomeMetric"]],
    c(0.2, 0.2, 0.1),
    tolerance = 1e-4)
  expect_equal(
    output_12026[["Threshold"]],
    c(0.27, 0.27, 0.27),
    tolerance = 1e-4)
  # Testing column names
  expect_equal(
    names(output_12026),
    c("Grp", "Observation", "SomeMetric", "Threshold"),
    fixed = TRUE)
  # Testing column classes
  expect_equal(
    xpectr::element_classes(output_12026),
    c("numeric", "numeric", "numeric", "numeric"),
    fixed = TRUE)
  # Testing column types
  expect_equal(
    xpectr::element_types(output_12026),
    c("double", "double", "double", "double"),
    fixed = TRUE)
  # Testing dimensions
  expect_equal(
    dim(output_12026),
    3:4)
  # Testing group keys
  expect_equal(
    colnames(dplyr::group_keys(output_12026)),
    character(0),
    fixed = TRUE)

  # Testing exceeds_threshold(data = df, threshold = 0.3, ...
  # Testing side effects
  expect_error(
    xpectr::strip_msg(exceeds_threshold(data = df, threshold = 0.3, threshold_is = "percentage", metric_name = "SomeMetric", maximize = TRUE, grouping_keys = NULL)),
    xpectr::strip(paste0("1 assertions failed:\n * Variable 'grouping_keys': Must be o",
           "f type 'data.frame', not 'NULL'.")),
    fixed = TRUE)

  # Testing exceeds_threshold(data = df, threshold = 0.3, ...
  # Testing side effects
  expect_error(
    xpectr::strip_msg(exceeds_threshold(data = df, threshold = 0.3, threshold_is = "percentage", metric_name = "SomeMetric", maximize = NULL, grouping_keys = data.frame())),
    xpectr::strip(paste0("1 assertions failed:\n * Variable 'maximize': Must be of typ",
           "e 'logical flag', not 'NULL'.")),
    fixed = TRUE)

  # Testing exceeds_threshold(data = df, threshold = 0.3, ...
  # Testing side effects
  expect_error(
    xpectr::strip_msg(exceeds_threshold(data = df, threshold = 0.3, threshold_is = "percentage", metric_name = NULL, maximize = TRUE, grouping_keys = data.frame())),
    xpectr::strip("invalid type/length (symbol/0) in vector allocation"),
    fixed = TRUE)

  # Testing exceeds_threshold(data = df, threshold = NULL,...
  # Testing side effects
  expect_error(
    xpectr::strip_msg(exceeds_threshold(data = df, threshold = NULL, threshold_is = "percentage", metric_name = "SomeMetric", maximize = TRUE, grouping_keys = data.frame())),
    xpectr::strip(paste0("1 assertions failed:\n * Variable 'threshold': Must be of ty",
           "pe 'number', not 'NULL'.")),
    fixed = TRUE)

  # Testing exceeds_threshold(data = df, threshold = 0.3, ...
  # Testing side effects
  expect_error(
    xpectr::strip_msg(exceeds_threshold(data = df, threshold = 0.3, threshold_is = NULL, metric_name = "SomeMetric", maximize = TRUE, grouping_keys = data.frame())),
    xpectr::strip(paste0("1 assertions failed:\n * Variable 'threshold_is': Must be a ",
           "subset of {'percentage','score'}, not 'NULL'.")),
    fixed = TRUE)

  ## Finished testing 'exceeds_threshold'                                     ####


  # Now with 'score' as baseline

  # xpectr::gxs_function(fn = exceeds_threshold,
  #                      args_values = list(
  #                        "data" = list(df, dplyr::group_by(df,.data$Grp), NA, list("a" = 3)),
  #                        "threshold" = list(7, 1, 0, NA, c(0.2, 0.4)),
  #                        "threshold_is" = list("score"),
  #                        "metric_name" = list("SomeMetric", "no", NA, 3),
  #                        "maximize" = list(TRUE, FALSE, NA, 1, "TRUE"),
  #                        "grouping_keys" = c(data.frame(),
  #                                            dplyr::group_keys(dplyr::group_by(df,.data$Grp)),
  #                                            NA)
  #                      ))

  ## Testing 'exceeds_threshold'                                              ####
  ## Initially generated by xpectr
  # Testing different combinations of argument values

  # Testing exceeds_threshold(data = dplyr::group_by(df, ....
  # Testing side effects
  expect_error(
    xpectr::strip_msg(exceeds_threshold(data = dplyr::group_by(df, .data$Grp), threshold = 7, threshold_is = "score", metric_name = "SomeMetric", maximize = TRUE, grouping_keys = data.frame())),
    xpectr::strip("1 assertions failed:\n * 'data' cannot be grouped at this stage."),
    fixed = TRUE)

  # Testing exceeds_threshold(data = df, threshold = 7, th...
  # Testing side effects
  expect_error(
    xpectr::strip_msg(exceeds_threshold(data = df, threshold = 7, threshold_is = "score", metric_name = 3, maximize = TRUE, grouping_keys = data.frame())),
    xpectr::strip(paste0("1 assertions failed:\n * Variable 'colnames(data)': Must inc",
           "lude the elements {3}.")),
    fixed = TRUE)

  # Testing exceeds_threshold(data = df, threshold = 7, th...
  # Assigning output
  output_17110 <- exceeds_threshold(data = df, threshold = 7, threshold_is = "score", metric_name = "SomeMetric", maximize = FALSE, grouping_keys = data.frame())
  # Testing class
  expect_equal(
    class(output_17110),
    "data.frame",
    fixed = TRUE)
  # Testing column values
  expect_equal(
    output_17110[["Grp"]],
    numeric(0),
    tolerance = 1e-4)
  expect_equal(
    output_17110[["Observation"]],
    numeric(0),
    tolerance = 1e-4)
  expect_equal(
    output_17110[["SomeMetric"]],
    numeric(0),
    tolerance = 1e-4)
  expect_equal(
    output_17110[["Threshold"]],
    numeric(0),
    tolerance = 1e-4)
  # Testing column names
  expect_equal(
    names(output_17110),
    c("Grp", "Observation", "SomeMetric", "Threshold"),
    fixed = TRUE)
  # Testing column classes
  expect_equal(
    xpectr::element_classes(output_17110),
    c("numeric", "numeric", "numeric", "numeric"),
    fixed = TRUE)
  # Testing column types
  expect_equal(
    xpectr::element_types(output_17110),
    c("double", "double", "double", "double"),
    fixed = TRUE)
  # Testing dimensions
  expect_equal(
    dim(output_17110),
    c(0L, 4L))
  # Testing group keys
  expect_equal(
    colnames(dplyr::group_keys(output_17110)),
    character(0),
    fixed = TRUE)

  # Testing exceeds_threshold(data = df, threshold = 7, th...
  # Testing side effects
  expect_error(
    xpectr::strip_msg(exceeds_threshold(data = df, threshold = 7, threshold_is = "score", metric_name = "SomeMetric", maximize = NA, grouping_keys = data.frame())),
    xpectr::strip("1 assertions failed:\n * Variable 'maximize': May not be NA."),
    fixed = TRUE)

  # Testing exceeds_threshold(data = df, threshold = 7, th...
  # Testing side effects
  expect_error(
    xpectr::strip_msg(exceeds_threshold(data = df, threshold = 7, threshold_is = "score", metric_name = "SomeMetric", maximize = 1, grouping_keys = data.frame())),
    xpectr::strip(paste0("1 assertions failed:\n * Variable 'maximize': Must be of typ",
           "e 'logical flag', not 'double'.")),
    fixed = TRUE)

  # Testing exceeds_threshold(data = df, threshold = 7, th...
  # Testing side effects
  expect_error(
    xpectr::strip_msg(exceeds_threshold(data = df, threshold = 7, threshold_is = "score", metric_name = "SomeMetric", maximize = "TRUE", grouping_keys = data.frame())),
    xpectr::strip(paste0("1 assertions failed:\n * Variable 'maximize': Must be of typ",
           "e 'logical flag', not 'character'.")),
    fixed = TRUE)

  # Testing exceeds_threshold(data = df, threshold = 7, th...
  # Assigning output
  output_11216 <- exceeds_threshold(data = df, threshold = 7, threshold_is = "score", metric_name = "SomeMetric", maximize = TRUE, grouping_keys = dplyr::group_keys(dplyr::group_by(df, .data$Grp)))
  # Testing class
  expect_equal(
    class(output_11216),
    "data.frame",
    fixed = TRUE)
  # Testing column values
  expect_equal(
    output_11216[["Grp"]],
    c(1, 1, 1, 1, 1, 2, 2, 2, 2, 2),
    tolerance = 1e-4)
  expect_equal(
    output_11216[["Observation"]],
    c(1, 2, 3, 4, 5, 1, 2, 3, 4, 5),
    tolerance = 1e-4)
  expect_equal(
    output_11216[["SomeMetric"]],
    c(0.2, 0.4, 0.6, 0.8, 0.9, 0.3, 0.2, 0.1, 0.8, 0.7),
    tolerance = 1e-4)
  expect_equal(
    output_11216[["Threshold"]],
    c(7, 7, 7, 7, 7, 7, 7, 7, 7, 7),
    tolerance = 1e-4)
  # Testing column names
  expect_equal(
    names(output_11216),
    c("Grp", "Observation", "SomeMetric", "Threshold"),
    fixed = TRUE)
  # Testing column classes
  expect_equal(
    xpectr::element_classes(output_11216),
    c("numeric", "numeric", "numeric", "numeric"),
    fixed = TRUE)
  # Testing column types
  expect_equal(
    xpectr::element_types(output_11216),
    c("double", "double", "double", "double"),
    fixed = TRUE)
  # Testing dimensions
  expect_equal(
    dim(output_11216),
    c(10L, 4L))
  # Testing group keys
  expect_equal(
    colnames(dplyr::group_keys(output_11216)),
    character(0),
    fixed = TRUE)

  # Testing exceeds_threshold(data = df, threshold = 7, th...
  # Testing side effects
  expect_error(
    xpectr::strip_msg(exceeds_threshold(data = df, threshold = 7, threshold_is = "score", metric_name = "SomeMetric", maximize = TRUE, grouping_keys = NA)),
    xpectr::strip(paste0("1 assertions failed:\n * Variable 'grouping_keys': Must be o",
           "f type 'data.frame', not 'logical'.")),
    fixed = TRUE)

  # Testing exceeds_threshold(data = NA, threshold = 7, th...
  # Testing side effects
  expect_error(
    xpectr::strip_msg(exceeds_threshold(data = NA, threshold = 7, threshold_is = "score", metric_name = "SomeMetric", maximize = TRUE, grouping_keys = data.frame())),
    xpectr::strip(paste0("1 assertions failed:\n * Variable 'data': Must be of type 'd",
           "ata.frame', not 'logical'.")),
    fixed = TRUE)

  # Testing exceeds_threshold(data = list(a = 3), threshol...
  # Testing side effects
  expect_error(
    xpectr::strip_msg(exceeds_threshold(data = list(a = 3), threshold = 7, threshold_is = "score", metric_name = "SomeMetric", maximize = TRUE, grouping_keys = data.frame())),
    xpectr::strip(paste0("1 assertions failed:\n * Variable 'data': Must be of type 'd",
           "ata.frame', not 'list'.")),
    fixed = TRUE)

  # Testing exceeds_threshold(data = df, threshold = 1, th...
  # Assigning output
  output_12454 <- exceeds_threshold(data = df, threshold = 1, threshold_is = "score", metric_name = "SomeMetric", maximize = TRUE, grouping_keys = data.frame())
  # Testing class
  expect_equal(
    class(output_12454),
    "data.frame",
    fixed = TRUE)
  # Testing column values
  expect_equal(
    output_12454[["Grp"]],
    c(1, 1, 1, 1, 1, 2, 2, 2, 2, 2),
    tolerance = 1e-4)
  expect_equal(
    output_12454[["Observation"]],
    c(1, 2, 3, 4, 5, 1, 2, 3, 4, 5),
    tolerance = 1e-4)
  expect_equal(
    output_12454[["SomeMetric"]],
    c(0.2, 0.4, 0.6, 0.8, 0.9, 0.3, 0.2, 0.1, 0.8, 0.7),
    tolerance = 1e-4)
  expect_equal(
    output_12454[["Threshold"]],
    c(1, 1, 1, 1, 1, 1, 1, 1, 1, 1),
    tolerance = 1e-4)
  # Testing column names
  expect_equal(
    names(output_12454),
    c("Grp", "Observation", "SomeMetric", "Threshold"),
    fixed = TRUE)
  # Testing column classes
  expect_equal(
    xpectr::element_classes(output_12454),
    c("numeric", "numeric", "numeric", "numeric"),
    fixed = TRUE)
  # Testing column types
  expect_equal(
    xpectr::element_types(output_12454),
    c("double", "double", "double", "double"),
    fixed = TRUE)
  # Testing dimensions
  expect_equal(
    dim(output_12454),
    c(10L, 4L))
  # Testing group keys
  expect_equal(
    colnames(dplyr::group_keys(output_12454)),
    character(0),
    fixed = TRUE)

  # Testing exceeds_threshold(data = df, threshold = 0, th...
  # Assigning output
  output_11432 <- exceeds_threshold(data = df, threshold = 0, threshold_is = "score", metric_name = "SomeMetric", maximize = TRUE, grouping_keys = data.frame())
  # Testing class
  expect_equal(
    class(output_11432),
    "data.frame",
    fixed = TRUE)
  # Testing column values
  expect_equal(
    output_11432[["Grp"]],
    numeric(0),
    tolerance = 1e-4)
  expect_equal(
    output_11432[["Observation"]],
    numeric(0),
    tolerance = 1e-4)
  expect_equal(
    output_11432[["SomeMetric"]],
    numeric(0),
    tolerance = 1e-4)
  expect_equal(
    output_11432[["Threshold"]],
    numeric(0),
    tolerance = 1e-4)
  # Testing column names
  expect_equal(
    names(output_11432),
    c("Grp", "Observation", "SomeMetric", "Threshold"),
    fixed = TRUE)
  # Testing column classes
  expect_equal(
    xpectr::element_classes(output_11432),
    c("numeric", "numeric", "numeric", "numeric"),
    fixed = TRUE)
  # Testing column types
  expect_equal(
    xpectr::element_types(output_11432),
    c("double", "double", "double", "double"),
    fixed = TRUE)
  # Testing dimensions
  expect_equal(
    dim(output_11432),
    c(0L, 4L))
  # Testing group keys
  expect_equal(
    colnames(dplyr::group_keys(output_11432)),
    character(0),
    fixed = TRUE)

  # Testing exceeds_threshold(data = df, threshold = NA, t...
  # Testing side effects
  expect_error(
    xpectr::strip_msg(exceeds_threshold(data = df, threshold = NA, threshold_is = "score", metric_name = "SomeMetric", maximize = TRUE, grouping_keys = data.frame())),
    xpectr::strip("1 assertions failed:\n * Variable 'threshold': May not be NA."),
    fixed = TRUE)

  # Testing exceeds_threshold(data = df, threshold = c(0.2...
  # Testing side effects
  expect_error(
    xpectr::strip_msg(exceeds_threshold(data = df, threshold = c(0.2, 0.4), threshold_is = "score", metric_name = "SomeMetric", maximize = TRUE, grouping_keys = data.frame())),
    xpectr::strip("1 assertions failed:\n * Variable 'threshold': Must have length 1."),
    fixed = TRUE)

  # Testing exceeds_threshold(data = df, threshold = 7, th...
  # Testing side effects
  expect_error(
    xpectr::strip_msg(exceeds_threshold(data = df, threshold = 7, threshold_is = "score", metric_name = "no", maximize = TRUE, grouping_keys = data.frame())),
    xpectr::strip(paste0("1 assertions failed:\n * Variable 'colnames(data)': Must inc",
           "lude the elements {no}.")),
    fixed = TRUE)

  # Testing exceeds_threshold(data = df, threshold = 7, th...
  # Testing side effects
  expect_error(
    xpectr::strip_msg(exceeds_threshold(data = df, threshold = 7, threshold_is = "score", metric_name = NA, maximize = TRUE, grouping_keys = data.frame())),
    xpectr::strip(paste0("Assertion on 'must.include' failed. May not contain missing ",
           "values, first at position 1.")),
    fixed = TRUE)

  # Testing exceeds_threshold(data = NULL, threshold = 7, ...
  # Testing side effects
  expect_error(
    xpectr::strip_msg(exceeds_threshold(data = NULL, threshold = 7, threshold_is = "score", metric_name = "SomeMetric", maximize = TRUE, grouping_keys = data.frame())),
    xpectr::strip(paste0("1 assertions failed:\n * Variable 'data': Must be of type 'd",
           "ata.frame', not 'NULL'.")),
    fixed = TRUE)

  # Testing exceeds_threshold(data = df, threshold = 7, th...
  # Assigning output
  output_12396 <- exceeds_threshold(data = df, threshold = 7, threshold_is = "score", metric_name = "SomeMetric", maximize = TRUE, grouping_keys = data.frame())
  # Testing class
  expect_equal(
    class(output_12396),
    "data.frame",
    fixed = TRUE)
  # Testing column values
  expect_equal(
    output_12396[["Grp"]],
    c(1, 1, 1, 1, 1, 2, 2, 2, 2, 2),
    tolerance = 1e-4)
  expect_equal(
    output_12396[["Observation"]],
    c(1, 2, 3, 4, 5, 1, 2, 3, 4, 5),
    tolerance = 1e-4)
  expect_equal(
    output_12396[["SomeMetric"]],
    c(0.2, 0.4, 0.6, 0.8, 0.9, 0.3, 0.2, 0.1, 0.8, 0.7),
    tolerance = 1e-4)
  expect_equal(
    output_12396[["Threshold"]],
    c(7, 7, 7, 7, 7, 7, 7, 7, 7, 7),
    tolerance = 1e-4)
  # Testing column names
  expect_equal(
    names(output_12396),
    c("Grp", "Observation", "SomeMetric", "Threshold"),
    fixed = TRUE)
  # Testing column classes
  expect_equal(
    xpectr::element_classes(output_12396),
    c("numeric", "numeric", "numeric", "numeric"),
    fixed = TRUE)
  # Testing column types
  expect_equal(
    xpectr::element_types(output_12396),
    c("double", "double", "double", "double"),
    fixed = TRUE)
  # Testing dimensions
  expect_equal(
    dim(output_12396),
    c(10L, 4L))
  # Testing group keys
  expect_equal(
    colnames(dplyr::group_keys(output_12396)),
    character(0),
    fixed = TRUE)

  # Testing exceeds_threshold(data = df, threshold = 7, th...
  # Testing side effects
  expect_error(
    xpectr::strip_msg(exceeds_threshold(data = df, threshold = 7, threshold_is = "score", metric_name = "SomeMetric", maximize = TRUE, grouping_keys = NULL)),
    xpectr::strip(paste0("1 assertions failed:\n * Variable 'grouping_keys': Must be o",
           "f type 'data.frame', not 'NULL'.")),
    fixed = TRUE)

  # Testing exceeds_threshold(data = df, threshold = 7, th...
  # Testing side effects
  expect_error(
    xpectr::strip_msg(exceeds_threshold(data = df, threshold = 7, threshold_is = "score", metric_name = "SomeMetric", maximize = NULL, grouping_keys = data.frame())),
    xpectr::strip(paste0("1 assertions failed:\n * Variable 'maximize': Must be of typ",
           "e 'logical flag', not 'NULL'.")),
    fixed = TRUE)

  # Testing exceeds_threshold(data = df, threshold = 7, th...
  # Testing side effects
  expect_error(
    xpectr::strip_msg(exceeds_threshold(data = df, threshold = 7, threshold_is = "score", metric_name = NULL, maximize = TRUE, grouping_keys = data.frame())),
    xpectr::strip("attempt to select less than one element in get1index"),
    fixed = TRUE)

  # Testing exceeds_threshold(data = df, threshold = NULL,...
  # Testing side effects
  expect_error(
    xpectr::strip_msg(exceeds_threshold(data = df, threshold = NULL, threshold_is = "score", metric_name = "SomeMetric", maximize = TRUE, grouping_keys = data.frame())),
    xpectr::strip(paste0("1 assertions failed:\n * Variable 'threshold': Must be of ty",
           "pe 'number', not 'NULL'.")),
    fixed = TRUE)

  # Testing exceeds_threshold(data = df, threshold = 7, th...
  # Testing side effects
  expect_error(
    xpectr::strip_msg(exceeds_threshold(data = df, threshold = 7, threshold_is = NULL, metric_name = "SomeMetric", maximize = TRUE, grouping_keys = data.frame())),
    xpectr::strip(paste0("1 assertions failed:\n * Variable 'threshold_is': Must be a ",
           "subset of {'percentage','score'}, not 'NULL'.")),
    fixed = TRUE)

  ## Finished testing 'exceeds_threshold'                                     ####


})
