library(cvms)
context("most_challenging()")

test_that("binomial model works with most_challenging()", {

  # Load data and fold it
  xpectr::set_test_seed(1)
  dat_ready <- participant.scores %>%
    dplyr::mutate(diagnosis = as.factor(diagnosis))
  dat_list <- groupdata2::fold(dat_ready,
    k = 4,
    num_fold_cols = 8,
    cat_col = "diagnosis",
    id_col = "participant"
  )

  CV_binom <- cross_validate(
    dat_list,
    formulas = c("diagnosis ~ score", "diagnosis ~ score + age"),
    fold_cols = paste0(".folds_", 1:8),
    family = "binomial"
  )

  collected_preds <- dplyr::bind_rows(CV_binom$Predictions, .id = "Model") %>%
    dplyr::group_by(.data$Model)

  hard_to_predict_perc <- most_challenging(collected_preds,
    threshold = 0.25,
    threshold_is = "percentage"
  )

  ## Testing 'hard_to_predict_perc'                                         ####
  ## Initially generated by xpectr
  # Testing class
  expect_equal(
    class(hard_to_predict_perc),
    c("grouped_df", "tbl_df", "tbl", "data.frame"),
    fixed = TRUE)
  # Testing column values
  expect_equal(
    hard_to_predict_perc[["Model"]],
    c("1", "1", "1", "1", "1", "1", "1", "1", "2", "2", "2", "2",
      "2", "2", "2", "2", "2", "2"),
    fixed = TRUE)
  expect_equal(
    hard_to_predict_perc[["Observation"]],
    c(1, 4, 5, 7, 10, 20, 21, 15, 1, 4, 7, 10, 20, 21, 30, 5, 11,
      15),
    tolerance = 1e-4)
  expect_equal(
    hard_to_predict_perc[["Correct"]],
    c(0, 0, 0, 0, 0, 0, 0, 6, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2),
    tolerance = 1e-4)
  expect_equal(
    hard_to_predict_perc[["Incorrect"]],
    c(8, 8, 8, 8, 8, 8, 8, 2, 8, 8, 8, 8, 8, 8, 8, 6, 6, 6),
    tolerance = 1e-4)
  expect_equal(
    hard_to_predict_perc[["Accuracy"]],
    c(0, 0, 0, 0, 0, 0, 0, 0.75, 0, 0, 0, 0, 0, 0, 0, 0.25, 0.25,
      0.25),
    tolerance = 1e-4)
  expect_equal(
    hard_to_predict_perc[["<="]],
    c(0.78125, 0.78125, 0.78125, 0.78125, 0.78125, 0.78125, 0.78125,
      0.78125, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25,
      0.25),
    tolerance = 1e-4)
  # Testing column names
  expect_equal(
    names(hard_to_predict_perc),
    c("Model", "Observation", "Correct", "Incorrect", "Accuracy",
      "<="),
    fixed = TRUE)
  # Testing column classes
  expect_equal(
    xpectr::element_classes(hard_to_predict_perc),
    c("character", "integer", "integer", "integer", "numeric", "numeric"),
    fixed = TRUE)
  # Testing column types
  expect_equal(
    xpectr::element_types(hard_to_predict_perc),
    c("character", "integer", "integer", "integer", "double", "double"),
    fixed = TRUE)
  # Testing dimensions
  expect_equal(
    dim(hard_to_predict_perc),
    c(18L, 6L))
  # Testing group keys
  expect_equal(
    colnames(dplyr::group_keys(hard_to_predict_perc)),
    "Model",
    fixed = TRUE)
  ## Finished testing 'hard_to_predict_perc'                                ####

  hard_to_predict_score <- most_challenging(collected_preds,threshold = 0.30, threshold_is = "score")

  ## Testing 'hard_to_predict_score'                                        ####
  ## Initially generated by xpectr
  # Testing class
  expect_equal(
    class(hard_to_predict_score),
    c("grouped_df", "tbl_df", "tbl", "data.frame"),
    fixed = TRUE)
  # Testing column values
  expect_equal(
    hard_to_predict_score[["Model"]],
    c("1", "1", "1", "1", "1", "1", "1", "2", "2", "2", "2", "2",
      "2", "2", "2", "2", "2"),
    fixed = TRUE)
  expect_equal(
    hard_to_predict_score[["Observation"]],
    c(1, 4, 5, 7, 10, 20, 21, 1, 4, 7, 10, 20, 21, 30, 5, 11, 15),
    tolerance = 1e-4)
  expect_equal(
    hard_to_predict_score[["Correct"]],
    c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2),
    tolerance = 1e-4)
  expect_equal(
    hard_to_predict_score[["Incorrect"]],
    c(8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 6, 6, 6),
    tolerance = 1e-4)
  expect_equal(
    hard_to_predict_score[["Accuracy"]],
    c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.25, 0.25, 0.25),
    tolerance = 1e-4)
  expect_equal(
    hard_to_predict_score[["<="]],
    c(0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3,
      0.3, 0.3, 0.3, 0.3, 0.3),
    tolerance = 1e-4)
  # Testing column names
  expect_equal(
    names(hard_to_predict_score),
    c("Model", "Observation", "Correct", "Incorrect", "Accuracy",
      "<="),
    fixed = TRUE)
  # Testing column classes
  expect_equal(
    xpectr::element_classes(hard_to_predict_score),
    c("character", "integer", "integer", "integer", "numeric", "numeric"),
    fixed = TRUE)
  # Testing column types
  expect_equal(
    xpectr::element_types(hard_to_predict_score),
    c("character", "integer", "integer", "integer", "double", "double"),
    fixed = TRUE)
  # Testing dimensions
  expect_equal(
    dim(hard_to_predict_score),
    c(17L, 6L))
  # Testing group keys
  expect_equal(
    colnames(dplyr::group_keys(hard_to_predict_score)),
    "Model",
    fixed = TRUE)
  ## Finished testing 'hard_to_predict_score'                               ####

})

test_that("multinomial model works with most_challenging()", {

  # Load data and fold it
  xpectr::set_test_seed(1)

  # # Create and fold dataset
  data_mc <- multiclass_probability_tibble(
    num_classes = 3, num_observations = 50,
    apply_softmax = TRUE, FUN = runif,
    class_name = "predictor_"
  )
  class_names <- paste0("class_", c(1, 2, 3))
  data_mc[["target"]] <- factor(sample(
    x = class_names,
    size = 50, replace = TRUE
  ))
  dat <- groupdata2::fold(data_mc, k = 4, num_fold_cols = 8)

  multinom_model_fn <- function(train_data, formula, hyperparameters) {
    nnet::multinom(
      formula = formula, # converted to formula object within fit_model()
      data = train_data
    )
  }

  random_predict_fn <- function(test_data, model, formula, hyperparameters) {
    multiclass_probability_tibble(
      num_classes = 3, num_observations = nrow(test_data),
      apply_softmax = TRUE, FUN = runif,
      class_name = "class_"
    )
  }

  CVmultinomlist <- cross_validate_fn(dat,
    model_fn = multinom_model_fn,
    predict_fn = random_predict_fn,
    formulas = c(
      "target ~ predictor_1 + predictor_2 + predictor_3",
      "target ~ predictor_1"
    ),
    fold_cols = paste0(".folds_", 1:8),
    type = "multinomial"
  )

  collected_preds <- dplyr::bind_rows(CVmultinomlist$Predictions, .id = "Model") %>%
    dplyr::group_by(.data$Model)

  hard_to_predict <- most_challenging(
    collected_preds,
    threshold = 0.15,
    threshold_is = "percentage",
    type = "multinomial"
  )

  ## Testing 'hard_to_predict'                                              ####
  ## Initially generated by xpectr
  # Testing class
  expect_equal(
    class(hard_to_predict),
    c("grouped_df", "tbl_df", "tbl", "data.frame"),
    fixed = TRUE)
  # Testing column values
  expect_equal(
    xpectr::smpl(hard_to_predict[["Model"]], n = 30),
    c("1", "1", "1", "1", "1", "1", "1", "1", "1", "1", "1", "2",
      "2", "2", "2", "2", "2", "2", "2", "2", "2", "2", "2", "2",
      "2", "2", "2", "2", "2", "2"),
    fixed = TRUE)
  expect_equal(
    xpectr::smpl(hard_to_predict[["Observation"]], n = 30),
    c(4, 10, 11, 12, 35, 8, 28, 31, 39, 47, 48, 33, 50, 2, 7, 10,
      11, 16, 18, 21, 23, 25, 27, 28, 29, 31, 37, 41, 43, 45),
    tolerance = 1e-4)
  expect_equal(
    xpectr::smpl(hard_to_predict[["Correct"]], n = 30),
    c(0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 2, 2, 2, 2, 2, 2, 2,
      2, 2, 2, 2, 2, 2, 2, 2, 2, 2),
    tolerance = 1e-4)
  expect_equal(
    xpectr::smpl(hard_to_predict[["Incorrect"]], n = 30),
    c(8, 8, 8, 8, 8, 7, 7, 7, 7, 7, 7, 8, 8, 6, 6, 6, 6, 6, 6, 6,
      6, 6, 6, 6, 6, 6, 6, 6, 6, 6),
    tolerance = 1e-4)
  expect_equal(
    xpectr::smpl(hard_to_predict[["Accuracy"]], n = 30),
    c(0, 0, 0, 0, 0, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0,
      0, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25,
      0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25),
    tolerance = 1e-4)
  expect_equal(
    xpectr::smpl(hard_to_predict[["<="]], n = 30),
    c(0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125,
      0.125, 0.125, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25,
      0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25,
      0.25),
    tolerance = 1e-4)
  # Testing column names
  expect_equal(
    names(hard_to_predict),
    c("Model", "Observation", "Correct", "Incorrect", "Accuracy",
      "<="),
    fixed = TRUE)
  # Testing column classes
  expect_equal(
    xpectr::element_classes(hard_to_predict),
    c("character", "integer", "integer", "integer", "numeric", "numeric"),
    fixed = TRUE)
  # Testing column types
  expect_equal(
    xpectr::element_types(hard_to_predict),
    c("character", "integer", "integer", "integer", "double", "double"),
    fixed = TRUE)
  # Testing dimensions
  expect_equal(
    dim(hard_to_predict),
    c(35L, 6L))
  # Testing group keys
  expect_equal(
    colnames(dplyr::group_keys(hard_to_predict)),
    "Model",
    fixed = TRUE)
  ## Finished testing 'hard_to_predict'                                     ####

  hard_to_predict <- most_challenging(collected_preds, threshold = 0.15, threshold_is = "score")

  ## Testing 'hard_to_predict'                                              ####
  ## Initially generated by xpectr
  # Testing class
  expect_equal(
    class(hard_to_predict),
    c("grouped_df", "tbl_df", "tbl", "data.frame"),
    fixed = TRUE)
  # Testing column values
  expect_equal(
    hard_to_predict[["Model"]],
    c("1", "1", "1", "1", "1", "1", "1", "1", "1", "1", "1", "1",
      "1", "1", "2", "2", "2"),
    fixed = TRUE)
  expect_equal(
    hard_to_predict[["Observation"]],
    c(4, 10, 11, 12, 17, 35, 6, 8, 28, 31, 33, 39, 47, 48, 5, 33,
      50),
    tolerance = 1e-4)
  expect_equal(
    hard_to_predict[["Correct"]],
    c(0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0),
    tolerance = 1e-4)
  expect_equal(
    hard_to_predict[["Incorrect"]],
    c(8, 8, 8, 8, 8, 8, 7, 7, 7, 7, 7, 7, 7, 7, 8, 8, 8),
    tolerance = 1e-4)
  expect_equal(
    hard_to_predict[["Accuracy"]],
    c(0, 0, 0, 0, 0, 0, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125,
      0.125, 0.125, 0, 0, 0),
    tolerance = 1e-4)
  expect_equal(
    hard_to_predict[["<="]],
    c(0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15,
      0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15),
    tolerance = 1e-4)
  # Testing column names
  expect_equal(
    names(hard_to_predict),
    c("Model", "Observation", "Correct", "Incorrect", "Accuracy",
      "<="),
    fixed = TRUE)
  # Testing column classes
  expect_equal(
    xpectr::element_classes(hard_to_predict),
    c("character", "integer", "integer", "integer", "numeric", "numeric"),
    fixed = TRUE)
  # Testing column types
  expect_equal(
    xpectr::element_types(hard_to_predict),
    c("character", "integer", "integer", "integer", "double", "double"),
    fixed = TRUE)
  # Testing dimensions
  expect_equal(
    dim(hard_to_predict),
    c(17L, 6L))
  # Testing group keys
  expect_equal(
    colnames(dplyr::group_keys(hard_to_predict)),
    "Model",
    fixed = TRUE)
  ## Finished testing 'hard_to_predict'                                     ####

})

test_that("multinomial - predicted.musicians works with most_challenging()", {

  xpectr::set_test_seed(1)

  hard_to_predict <- most_challenging(
    predicted.musicians %>% dplyr::group_by(.data$Classifier),
    obs_id_col = "ID",
    threshold = 0.15,
    threshold_is = "percentage",
    type = "multinomial"
  )

  ## Testing 'hard_to_predict'                                              ####
  ## Initially generated by xpectr
  # Testing class
  expect_equal(
    class(hard_to_predict),
    c("grouped_df", "tbl_df", "tbl", "data.frame"),
    fixed = TRUE)
  # Testing column values
  expect_equal(
    xpectr::smpl(hard_to_predict[["Classifier"]], n = 30),
    c("e1071_svm", "e1071_svm", "e1071_svm", "e1071_svm", "e1071_svm",
      "e1071_svm", "e1071_svm", "e1071_svm", "e1071_svm", "nnet_multinom",
      "nnet_multinom", "nnet_multinom", "nnet_multinom", "nnet_multinom",
      "nnet_multinom", "nnet_multinom", "nnet_multinom", "nnet_multinom",
      "nnet_multinom", "nnet_multinom", "randomForest", "randomForest",
      "randomForest", "randomForest", "randomForest", "randomForest",
      "randomForest", "randomForest", "randomForest", "randomForest"),
    fixed = TRUE)
  expect_equal(
    xpectr::smpl(hard_to_predict[["ID"]], n = 30),
    structure(c(7L, 17L, 22L, 27L, 40L, 48L, 51L, 52L, 59L, 1L, 2L,
      9L, 25L, 27L, 40L, 44L, 51L, 54L, 58L, 60L, 11L, 14L, 25L,
      30L, 43L, 47L, 50L, 51L, 52L, 60L), .Label = c("1", "2", "3",
      "4", "5", "6", "7", "8", "9", "10", "11", "12", "13", "14",
      "15", "16", "17", "18", "19", "20", "21", "22", "23", "24",
      "25", "26", "27", "28", "29", "30", "31", "32", "33", "34",
      "35", "36", "37", "38", "39", "40", "41", "42", "43", "44",
      "45", "46", "47", "48", "49", "50", "51", "52", "53", "54",
      "55", "56", "57", "58", "59", "60"), class = "factor"))
  expect_equal(
    xpectr::smpl(hard_to_predict[["Correct"]], n = 30),
    c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
      0, 0, 0, 0, 0, 0, 0, 0, 0, 0),
    tolerance = 1e-4)
  expect_equal(
    xpectr::smpl(hard_to_predict[["Incorrect"]], n = 30),
    c(3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
      3, 3, 3, 3, 3, 3, 3, 3, 3, 3),
    tolerance = 1e-4)
  expect_equal(
    xpectr::smpl(hard_to_predict[["Accuracy"]], n = 30),
    c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
      0, 0, 0, 0, 0, 0, 0, 0, 0, 0),
    tolerance = 1e-4)
  expect_equal(
    xpectr::smpl(hard_to_predict[["<="]], n = 30),
    c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
      0, 0, 0, 0, 0, 0, 0, 0, 0, 0),
    tolerance = 1e-4)
  # Testing column names
  expect_equal(
    names(hard_to_predict),
    c("Classifier", "ID", "Correct", "Incorrect", "Accuracy", "<="),
    fixed = TRUE)
  # Testing column classes
  expect_equal(
    xpectr::element_classes(hard_to_predict),
    c("character", "factor", "integer", "integer", "numeric", "numeric"),
    fixed = TRUE)
  # Testing column types
  expect_equal(
    xpectr::element_types(hard_to_predict),
    c("character", "integer", "integer", "integer", "double", "double"),
    fixed = TRUE)
  # Testing dimensions
  expect_equal(
    dim(hard_to_predict),
    c(67L, 6L))
  # Testing group keys
  expect_equal(
    colnames(dplyr::group_keys(hard_to_predict)),
    "Classifier",
    fixed = TRUE)
  ## Finished testing 'hard_to_predict'                                     ####

  hard_to_predict_score <- most_challenging(
    predicted.musicians %>% dplyr::group_by(.data$Classifier),
    obs_id_col = "ID",
    threshold = 0.4,
    threshold_is = "score",
    type = "multinomial"
  )

  ## Testing 'hard_to_predict_score'                                        ####
  ## Initially generated by xpectr
  # Testing class
  expect_equal(
    class(hard_to_predict_score),
    c("grouped_df", "tbl_df", "tbl", "data.frame"),
    fixed = TRUE)
  # Testing column values
  expect_equal(
    xpectr::smpl(hard_to_predict_score[["Classifier"]], n = 30),
    c("e1071_svm", "e1071_svm", "e1071_svm", "e1071_svm", "e1071_svm",
      "e1071_svm", "e1071_svm", "nnet_multinom", "nnet_multinom",
      "nnet_multinom", "nnet_multinom", "nnet_multinom", "nnet_multinom",
      "nnet_multinom", "nnet_multinom", "nnet_multinom", "nnet_multinom",
      "randomForest", "randomForest", "randomForest", "randomForest",
      "randomForest", "randomForest", "randomForest", "randomForest",
      "randomForest", "randomForest", "randomForest", "randomForest",
      "randomForest"),
    fixed = TRUE)
  expect_equal(
    xpectr::smpl(hard_to_predict_score[["ID"]], n = 30),
    structure(c(22L, 29L, 30L, 37L, 59L, 26L, 50L, 17L, 22L, 24L,
      30L, 44L, 12L, 35L, 39L, 52L, 53L, 1L, 14L, 25L, 29L, 43L,
      49L, 50L, 51L, 60L, 12L, 15L, 40L, 41L), .Label = c("1", "2",
      "3", "4", "5", "6", "7", "8", "9", "10", "11", "12", "13",
      "14", "15", "16", "17", "18", "19", "20", "21", "22", "23",
      "24", "25", "26", "27", "28", "29", "30", "31", "32", "33",
      "34", "35", "36", "37", "38", "39", "40", "41", "42", "43",
      "44", "45", "46", "47", "48", "49", "50", "51", "52", "53",
      "54", "55", "56", "57", "58", "59", "60"), class = "factor"))
  expect_equal(
    xpectr::smpl(hard_to_predict_score[["Correct"]], n = 30),
    c(0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0,
      0, 0, 0, 0, 0, 0, 1, 1, 1, 1),
    tolerance = 1e-4)
  expect_equal(
    xpectr::smpl(hard_to_predict_score[["Incorrect"]], n = 30),
    c(3, 3, 3, 3, 3, 2, 2, 3, 3, 3, 3, 3, 2, 2, 2, 2, 2, 3, 3, 3,
      3, 3, 3, 3, 3, 3, 2, 2, 2, 2),
    tolerance = 1e-4)
  expect_equal(
    xpectr::smpl(hard_to_predict_score[["Accuracy"]], n = 30),
    c(0, 0, 0, 0, 0, 0.33333, 0.33333, 0, 0, 0, 0, 0, 0.33333, 0.33333,
      0.33333, 0.33333, 0.33333, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.33333,
      0.33333, 0.33333, 0.33333),
    tolerance = 1e-4)
  expect_equal(
    xpectr::smpl(hard_to_predict_score[["<="]], n = 30),
    c(0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4,
      0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4,
      0.4, 0.4, 0.4, 0.4, 0.4, 0.4),
    tolerance = 1e-4)
  # Testing column names
  expect_equal(
    names(hard_to_predict_score),
    c("Classifier", "ID", "Correct", "Incorrect", "Accuracy", "<="),
    fixed = TRUE)
  # Testing column classes
  expect_equal(
    xpectr::element_classes(hard_to_predict_score),
    c("character", "factor", "integer", "integer", "numeric", "numeric"),
    fixed = TRUE)
  # Testing column types
  expect_equal(
    xpectr::element_types(hard_to_predict_score),
    c("character", "integer", "integer", "integer", "double", "double"),
    fixed = TRUE)
  # Testing dimensions
  expect_equal(
    dim(hard_to_predict_score),
    c(97L, 6L))
  # Testing group keys
  expect_equal(
    colnames(dplyr::group_keys(hard_to_predict_score)),
    "Classifier",
    fixed = TRUE)
  ## Finished testing 'hard_to_predict_score'                               ####

  hard_to_predict_perc_non_grouped <- most_challenging(
    predicted.musicians,
    obs_id_col = "ID",
    threshold = 0.30,
    threshold_is = "percentage",
    type = "multinomial"
  )

  ## Testing 'hard_to_predict_perc_non_grouped'                             ####
  ## Initially generated by xpectr
  # Testing class
  expect_equal(
    class(hard_to_predict_perc_non_grouped),
    c("tbl_df", "tbl", "data.frame"),
    fixed = TRUE)
  # Testing column values
  expect_equal(
    hard_to_predict_perc_non_grouped[["ID"]],
    structure(c(1L, 2L, 9L, 25L, 29L, 30L, 37L, 43L, 48L, 51L, 54L,
      58L, 59L, 60L, 7L, 14L, 22L, 27L, 35L, 40L, 52L), .Label = c("1",
      "2", "3", "4", "5", "6", "7", "8", "9", "10", "11", "12",
      "13", "14", "15", "16", "17", "18", "19", "20", "21", "22",
      "23", "24", "25", "26", "27", "28", "29", "30", "31", "32",
      "33", "34", "35", "36", "37", "38", "39", "40", "41", "42",
      "43", "44", "45", "46", "47", "48", "49", "50", "51", "52",
      "53", "54", "55", "56", "57", "58", "59", "60"), class = "factor"))
  expect_equal(
    hard_to_predict_perc_non_grouped[["Correct"]],
    c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1,
      1),
    tolerance = 1e-4)
  expect_equal(
    hard_to_predict_perc_non_grouped[["Incorrect"]],
    c(9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 8, 8, 8, 8, 8, 8,
      8),
    tolerance = 1e-4)
  expect_equal(
    hard_to_predict_perc_non_grouped[["Accuracy"]],
    c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.11111, 0.11111,
      0.11111, 0.11111, 0.11111, 0.11111, 0.11111),
    tolerance = 1e-4)
  expect_equal(
    hard_to_predict_perc_non_grouped[["<="]],
    c(0.11111, 0.11111, 0.11111, 0.11111, 0.11111, 0.11111, 0.11111,
      0.11111, 0.11111, 0.11111, 0.11111, 0.11111, 0.11111, 0.11111,
      0.11111, 0.11111, 0.11111, 0.11111, 0.11111, 0.11111, 0.11111),
    tolerance = 1e-4)
  # Testing column names
  expect_equal(
    names(hard_to_predict_perc_non_grouped),
    c("ID", "Correct", "Incorrect", "Accuracy", "<="),
    fixed = TRUE)
  # Testing column classes
  expect_equal(
    xpectr::element_classes(hard_to_predict_perc_non_grouped),
    c("factor", "integer", "integer", "numeric", "numeric"),
    fixed = TRUE)
  # Testing column types
  expect_equal(
    xpectr::element_types(hard_to_predict_perc_non_grouped),
    c("integer", "integer", "integer", "double", "double"),
    fixed = TRUE)
  # Testing dimensions
  expect_equal(
    dim(hard_to_predict_perc_non_grouped),
    c(21L, 5L))
  # Testing group keys
  expect_equal(
    colnames(dplyr::group_keys(hard_to_predict_perc_non_grouped)),
    character(0),
    fixed = TRUE)
  ## Finished testing 'hard_to_predict_perc_non_grouped'                    ####


  hard_to_predict_score_non_grouped <- most_challenging(
    predicted.musicians,
    obs_id_col = "ID",
    threshold = 0.4,
    threshold_is = "score",
    type = "multinomial"
  )

  ## Testing 'hard_to_predict_score_non_grouped'                            ####
  ## Initially generated by xpectr
  # Testing class
  expect_equal(
    class(hard_to_predict_score_non_grouped),
    c("tbl_df", "tbl", "data.frame"),
    fixed = TRUE)
  # Testing column values
  expect_equal(
    hard_to_predict_score_non_grouped[["ID"]],
    structure(c(1L, 2L, 9L, 25L, 29L, 30L, 37L, 43L, 48L, 51L, 54L,
      58L, 59L, 60L, 7L, 14L, 22L, 27L, 35L, 40L, 52L, 11L, 47L,
      4L, 17L, 44L, 49L, 53L), .Label = c("1", "2", "3", "4", "5",
      "6", "7", "8", "9", "10", "11", "12", "13", "14", "15", "16",
      "17", "18", "19", "20", "21", "22", "23", "24", "25", "26",
      "27", "28", "29", "30", "31", "32", "33", "34", "35", "36",
      "37", "38", "39", "40", "41", "42", "43", "44", "45", "46",
      "47", "48", "49", "50", "51", "52", "53", "54", "55", "56",
      "57", "58", "59", "60"), class = "factor"))
  expect_equal(
    hard_to_predict_score_non_grouped[["Correct"]],
    c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1,
      1, 2, 2, 3, 3, 3, 3, 3),
    tolerance = 1e-4)
  expect_equal(
    hard_to_predict_score_non_grouped[["Incorrect"]],
    c(9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 8, 8, 8, 8, 8, 8,
      8, 7, 7, 6, 6, 6, 6, 6),
    tolerance = 1e-4)
  expect_equal(
    hard_to_predict_score_non_grouped[["Accuracy"]],
    c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.11111, 0.11111,
      0.11111, 0.11111, 0.11111, 0.11111, 0.11111, 0.22222, 0.22222,
      0.33333, 0.33333, 0.33333, 0.33333, 0.33333),
    tolerance = 1e-4)
  expect_equal(
    hard_to_predict_score_non_grouped[["<="]],
    c(0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4,
      0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4,
      0.4, 0.4, 0.4, 0.4),
    tolerance = 1e-4)
  # Testing column names
  expect_equal(
    names(hard_to_predict_score_non_grouped),
    c("ID", "Correct", "Incorrect", "Accuracy", "<="),
    fixed = TRUE)
  # Testing column classes
  expect_equal(
    xpectr::element_classes(hard_to_predict_score_non_grouped),
    c("factor", "integer", "integer", "numeric", "numeric"),
    fixed = TRUE)
  # Testing column types
  expect_equal(
    xpectr::element_types(hard_to_predict_score_non_grouped),
    c("integer", "integer", "integer", "double", "double"),
    fixed = TRUE)
  # Testing dimensions
  expect_equal(
    dim(hard_to_predict_score_non_grouped),
    c(28L, 5L))
  # Testing group keys
  expect_equal(
    colnames(dplyr::group_keys(hard_to_predict_score_non_grouped)),
    character(0),
    fixed = TRUE)
  ## Finished testing 'hard_to_predict_score_non_grouped'                   ####

})

test_that("gaussian model works with most_challenging()", {

  # Load data and fold it
  xpectr::set_test_seed(1)
  dat_ready <- participant.scores %>%
    dplyr::mutate(diagnosis = as.factor(diagnosis))
  dat_list <- groupdata2::fold(dat_ready,
    k = 4,
    num_fold_cols = 8,
    cat_col = "diagnosis",
    id_col = "participant"
  )

  CV_gauss <- cross_validate(
    dat_list,
    formulas = c(
      "score ~ diagnosis + session",
      "score ~ diagnosis + age + session"
    ),
    fold_cols = paste0(".folds_", 1:8),
    family = "gaussian"
  )

  collected_preds <- dplyr::bind_rows(CV_gauss$Predictions, .id = "Model") %>%
    dplyr::group_by(.data$Model)

  hard_to_predict_perc <- most_challenging(collected_preds,
    type = "gaussian",
    threshold = 0.2, threshold_is = "percentage"
  )

  ## Testing 'hard_to_predict_perc'                                         ####
  ## Initially generated by xpectr
  # Testing class
  expect_equal(
    class(hard_to_predict_perc),
    c("grouped_df", "tbl_df", "tbl", "data.frame"),
    fixed = TRUE)
  # Testing column values
  expect_equal(
    hard_to_predict_perc[["Model"]],
    c("1", "1", "1", "1", "1", "1", "2", "2", "2", "2", "2", "2"),
    fixed = TRUE)
  expect_equal(
    hard_to_predict_perc[["Observation"]],
    c(20, 24, 21, 3, 5, 19, 20, 24, 21, 27, 3, 9),
    tolerance = 1e-4)
  expect_equal(
    hard_to_predict_perc[["MAE"]],
    c(26.61042, 20.93735, 18.2131, 14.77034, 13.33333, 13.00774, 26.76931,
      21.74228, 18.37199, 13.48558, 13.89225, 13.74331),
    tolerance = 1e-4)
  expect_equal(
    hard_to_predict_perc[["RMSE"]],
    c(26.61253, 20.95156, 18.22234, 14.78232, 13.33333, 13.01421,
      26.77572, 21.78456, 18.39316, 14.71466, 13.953, 13.82658),
    tolerance = 1e-4)
  expect_equal(
    hard_to_predict_perc[[">="]],
    c(12.61127, 12.61127, 12.61127, 12.61127, 12.61127, 12.61127,
      13.30434, 13.30434, 13.30434, 13.30434, 13.30434, 13.30434),
    tolerance = 1e-4)
  # Testing column names
  expect_equal(
    names(hard_to_predict_perc),
    c("Model", "Observation", "MAE", "RMSE", ">="),
    fixed = TRUE)
  # Testing column classes
  expect_equal(
    xpectr::element_classes(hard_to_predict_perc),
    c("character", "integer", "numeric", "numeric", "numeric"),
    fixed = TRUE)
  # Testing column types
  expect_equal(
    xpectr::element_types(hard_to_predict_perc),
    c("character", "integer", "double", "double", "double"),
    fixed = TRUE)
  # Testing dimensions
  expect_equal(
    dim(hard_to_predict_perc),
    c(12L, 5L))
  # Testing group keys
  expect_equal(
    colnames(dplyr::group_keys(hard_to_predict_perc)),
    "Model",
    fixed = TRUE)
  ## Finished testing 'hard_to_predict_perc'                                ####

  hard_to_predict_score <- most_challenging(collected_preds,
    type = "gaussian",
    threshold = 13.9, threshold_is = "score"
  )

  ## Testing 'hard_to_predict_score'                                        ####
  ## Initially generated by xpectr
  # Testing class
  expect_equal(
    class(hard_to_predict_score),
    c("grouped_df", "tbl_df", "tbl", "data.frame"),
    fixed = TRUE)
  # Testing column values
  expect_equal(
    hard_to_predict_score[["Model"]],
    c("1", "1", "1", "1", "2", "2", "2", "2", "2"),
    fixed = TRUE)
  expect_equal(
    hard_to_predict_score[["Observation"]],
    c(20, 24, 21, 3, 20, 24, 21, 27, 3),
    tolerance = 1e-4)
  expect_equal(
    hard_to_predict_score[["MAE"]],
    c(26.61042, 20.93735, 18.2131, 14.77034, 26.76931, 21.74228, 18.37199,
      13.48558, 13.89225),
    tolerance = 1e-4)
  expect_equal(
    hard_to_predict_score[["RMSE"]],
    c(26.61253, 20.95156, 18.22234, 14.78232, 26.77572, 21.78456,
      18.39316, 14.71466, 13.953),
    tolerance = 1e-4)
  expect_equal(
    hard_to_predict_score[[">="]],
    c(13.9, 13.9, 13.9, 13.9, 13.9, 13.9, 13.9, 13.9, 13.9),
    tolerance = 1e-4)
  # Testing column names
  expect_equal(
    names(hard_to_predict_score),
    c("Model", "Observation", "MAE", "RMSE", ">="),
    fixed = TRUE)
  # Testing column classes
  expect_equal(
    xpectr::element_classes(hard_to_predict_score),
    c("character", "integer", "numeric", "numeric", "numeric"),
    fixed = TRUE)
  # Testing column types
  expect_equal(
    xpectr::element_types(hard_to_predict_score),
    c("character", "integer", "double", "double", "double"),
    fixed = TRUE)
  # Testing dimensions
  expect_equal(
    dim(hard_to_predict_score),
    c(9L, 5L))
  # Testing group keys
  expect_equal(
    colnames(dplyr::group_keys(hard_to_predict_score)),
    "Model",
    fixed = TRUE)
  ## Finished testing 'hard_to_predict_score'                               ####

})
